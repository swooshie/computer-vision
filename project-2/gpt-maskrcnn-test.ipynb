{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d21b13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Setting up datasets...\n",
      "Filtering dataset in kaggle-data/train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:02<00:00, 73.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 209 images with valid annotations out of 209 total.\n",
      "Filtering dataset in kaggle-data/val...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 80.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45 images with valid annotations out of 45 total.\n",
      "Initializing model...\n",
      "Loading best model from nuclei_maskrcnn_model_cpu_BEST.pth for prediction...\n",
      "Starting prediction sweep... saving results to combinations-maskrcnn-final\n",
      "--- Running prediction for conf_thresh=0.1, mask_thresh=0.4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:25<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combinations-maskrcnn-final/submission_conf0.1_mask0.4.csv\n",
      "--- Running prediction for conf_thresh=0.1, mask_thresh=0.5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:12<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combinations-maskrcnn-final/submission_conf0.1_mask0.5.csv\n",
      "--- Running prediction for conf_thresh=0.25, mask_thresh=0.4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:25<00:00,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combinations-maskrcnn-final/submission_conf0.25_mask0.4.csv\n",
      "--- Running prediction for conf_thresh=0.25, mask_thresh=0.5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:25<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combinations-maskrcnn-final/submission_conf0.25_mask0.5.csv\n",
      "--- Running prediction for conf_thresh=0.4, mask_thresh=0.4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:27<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combinations-maskrcnn-final/submission_conf0.4_mask0.4.csv\n",
      "--- Running prediction for conf_thresh=0.4, mask_thresh=0.5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:27<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combinations-maskrcnn-final/submission_conf0.4_mask0.5.csv\n",
      "--- Running prediction for conf_thresh=0.5, mask_thresh=0.4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:25<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combinations-maskrcnn-final/submission_conf0.5_mask0.4.csv\n",
      "--- Running prediction for conf_thresh=0.5, mask_thresh=0.5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:24<00:00,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combinations-maskrcnn-final/submission_conf0.5_mask0.5.csv\n",
      "--- Prediction sweep finished! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as T\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set main data directory\n",
    "DATA_DIR = Path(\"kaggle-data\")\n",
    "TRAIN_IMG_DIR = DATA_DIR / \"train\"\n",
    "VAL_IMG_DIR = DATA_DIR / \"val\"\n",
    "TEST_IMG_DIR = DATA_DIR / \"test_final\"\n",
    "\n",
    "# Model parameters\n",
    "NUM_CLASSES = 5 # 4 classes + 1 background\n",
    "BATCH_SIZE = 2 # Keep this low for CPU training, 1 or 2 is good.\n",
    "NUM_EPOCHS = 30 # A more realistic number for starting to see results.\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "# Class mapping\n",
    "CLASS_MAP = {\n",
    "    \"Epithelial\": 1,\n",
    "    \"Lymphocyte\": 2,\n",
    "    \"Macrophage\": 3,\n",
    "    \"Neutrophil\": 4,\n",
    "}\n",
    "# Create an inverse map for prediction\n",
    "INV_CLASS_MAP = {v: k for k, v in CLASS_MAP.items()}\n",
    "\n",
    "# --- KEY FIX (Device) ---\n",
    "# Force CPU to avoid MPS bugs on M-series chips.\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# --- RLE Encoding Function (from problem description) ---\n",
    "def rle_encode_instance_mask(mask: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Convert an instance segmentation mask (H,W) -> RLE triple string.\n",
    "    0 = background, >0 = instance IDs.\n",
    "    \"\"\"\n",
    "    pixels = mask.flatten(order=\"F\").astype(np.int32)\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    rle = []\n",
    "    for i in range(len(runs) - 1):\n",
    "        start = runs[i]\n",
    "        end = runs[i + 1]\n",
    "        length = end - start\n",
    "        val = pixels[start]\n",
    "        if val > 0:\n",
    "            rle.extend([val, start, length])\n",
    "    if not rle:\n",
    "        return \"0\"\n",
    "    return \" \".join(map(str, rle))\n",
    "\n",
    "\n",
    "# --- Custom Dataset Class (Verified Logic) ---\n",
    "class NucleiDataset(Dataset):\n",
    "    def __init__(self, image_dir, transforms=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        all_image_files = sorted([f for f in self.image_dir.glob(\"*.tif\")])\n",
    "        all_xml_files = sorted([f for f in self.image_dir.glob(\"*.xml\")])\n",
    "\n",
    "        self.image_files = []\n",
    "        self.xml_files = []\n",
    "        \n",
    "        print(f\"Filtering dataset in {image_dir}...\")\n",
    "        for img_path, xml_path in tqdm(zip(all_image_files, all_xml_files), total=len(all_image_files)):\n",
    "            try:\n",
    "                tree = ET.parse(xml_path)\n",
    "                root = tree.getroot()\n",
    "                \n",
    "                # --- KEY FIX (Correct Hierarchy Parsing) ---\n",
    "                has_valid_annotation = False\n",
    "                \n",
    "                # Loop over top-level Annotations\n",
    "                for annotation in root.findall(\"Annotation\"):\n",
    "                    # Find the class name for this ANNOTATION\n",
    "                    attribute = annotation.find(\"Attributes/Attribute\")\n",
    "                    if attribute is None:\n",
    "                        continue\n",
    "                    \n",
    "                    label_name = attribute.get(\"Name\")\n",
    "                    if label_name in CLASS_MAP:\n",
    "                        # This annotation is for a class we care about.\n",
    "                        # Does it contain any regions (instances)?\n",
    "                        if annotation.find(\"Regions/Region\") is not None:\n",
    "                             has_valid_annotation = True\n",
    "                             break # Found a valid class with at least one region, this file is good.\n",
    "                \n",
    "                if has_valid_annotation:\n",
    "                    self.image_files.append(img_path)\n",
    "                    self.xml_files.append(xml_path)\n",
    "            except ET.ParseError:\n",
    "                print(f\"Warning: Skipping corrupted XML file: {xml_path}\")\n",
    "        \n",
    "        print(f\"Found {len(self.image_files)} images with valid annotations out of {len(all_image_files)} total.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = self.image_files[idx]\n",
    "            xml_path = self.xml_files[idx]\n",
    "            \n",
    "            # Load image\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            width, height = img.size\n",
    "\n",
    "            # Parse XML annotations\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            masks, labels, boxes = [], [], []\n",
    "            \n",
    "            # --- KEY FIX (Correct Hierarchy Parsing) ---\n",
    "            # Loop over top-level Annotations\n",
    "            for annotation in root.findall(\"Annotation\"):\n",
    "                # Find the class name for this ANNOTATION\n",
    "                attribute = annotation.find(\"Attributes/Attribute\")\n",
    "                if attribute is None:\n",
    "                    continue\n",
    "                    \n",
    "                label_name = attribute.get(\"Name\")\n",
    "                if label_name not in CLASS_MAP:\n",
    "                    continue\n",
    "                \n",
    "                # This is a class we care about, e.g., \"Epithelial\"\n",
    "                label_id = CLASS_MAP[label_name]\n",
    "\n",
    "                # Now, find all regions *within this annotation's Regions tag*\n",
    "                for region in annotation.findall(\"Regions/Region\"):\n",
    "                    vertices = []\n",
    "                    # Try the path from slide1.xml: Region -> Vertices -> Vertex\n",
    "                    vertex_nodes = region.findall(\"Vertices/Vertex\")\n",
    "                    \n",
    "                    if not vertex_nodes:\n",
    "                        # If that fails, try the other common path: Region -> Vertex\n",
    "                        vertex_nodes = region.findall(\"Vertex\")\n",
    "\n",
    "                    for vertex in vertex_nodes:\n",
    "                        x = float(vertex.get(\"X\"))\n",
    "                        y = float(vertex.get(\"Y\"))\n",
    "                        vertices.append((x, y))\n",
    "\n",
    "                    if not vertices:\n",
    "                        # This region has a class but no vertices, skip it.\n",
    "                        continue\n",
    "\n",
    "                    # Create a binary mask for this single instance\n",
    "                    instance_mask = Image.new(\"L\", (width, height), 0)\n",
    "                    ImageDraw.Draw(instance_mask).polygon(vertices, outline=1, fill=1)\n",
    "                    mask_np = np.array(instance_mask)\n",
    "                    \n",
    "                    # Get bounding box from mask\n",
    "                    pos = np.where(mask_np)\n",
    "                    if pos[0].size == 0 or pos[1].size == 0:\n",
    "                        continue # Skip empty masks\n",
    "                    xmin, xmax = np.min(pos[1]), np.max(pos[1])\n",
    "                    ymin, ymax = np.min(pos[0]), np.max(pos[0])\n",
    "                    \n",
    "                    # Check for valid box area\n",
    "                    if xmax > xmin and ymax > ymin:\n",
    "                        masks.append(mask_np)\n",
    "                        labels.append(label_id)\n",
    "                        boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            if not boxes: # This should now only happen if a \"valid\" file has 0-area polygons\n",
    "                target = {\n",
    "                    \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n",
    "                    \"labels\": torch.zeros(0, dtype=torch.int64),\n",
    "                    \"masks\": torch.zeros((0, height, width), dtype=torch.uint8)\n",
    "                }\n",
    "            else:\n",
    "                # Convert to tensors\n",
    "                boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "                labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "                masks = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n",
    "\n",
    "                target = {\"boxes\": boxes, \"labels\": labels, \"masks\": masks}\n",
    "            \n",
    "            # Apply transforms\n",
    "            img_tensor = T.ToImage()(img) # Convert PIL to tensor\n",
    "            img_tensor = T.ToDtype(torch.float32, scale=True)(img_tensor) # Normalize to [0,1]\n",
    "            \n",
    "            if self.transforms:\n",
    "                # Apply augmentations if any (e.g., RandomHorizontalFlip)\n",
    "                # Note: v2 transforms update target in-place\n",
    "                img_tensor, target = self.transforms(img_tensor, target)\n",
    "\n",
    "            return img_tensor, target\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading item at index {idx}, path: {self.image_files[idx]}. Error: {e}\")\n",
    "            return None, None # Return None to be filtered by collate_fn\n",
    "\n",
    "# --- Data Augmentation ---\n",
    "def get_transforms(is_train):\n",
    "    transforms = []\n",
    "    # ToImage() and ToDtype() are now handled in __getitem__\n",
    "    if is_train:\n",
    "        # Adds random horizontal flipping for augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "        # --- ADD MORE AUGMENTATIONS HERE ---\n",
    "        # transforms.append(T.RandomVerticalFlip(0.5))\n",
    "        # transforms.append(T.RandomRotation(degrees=30))\n",
    "    \n",
    "    # --- FIX: Return None if transforms list is empty ---\n",
    "    if not transforms:\n",
    "        return None\n",
    "    \n",
    "    return T.Compose(transforms)\n",
    "\n",
    "# --- Model Definition ---\n",
    "def get_model(num_classes):\n",
    "    # Load a pre-trained instance segmentation model\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # Replace the box predictor\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Replace the mask predictor\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- Utility for DataLoader ---\n",
    "def collate_fn(batch):\n",
    "    # Filter out None entries (from errors in __getitem__)\n",
    "    batch = [(img, tgt) for img, tgt in batch if img is not None and tgt is not None]\n",
    "    if not batch:\n",
    "        return None, None # Return None to be skipped in training loop\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    loop = tqdm(data_loader, leave=True)\n",
    "    total_loss = 0\n",
    "    batches_processed = 0\n",
    "    \n",
    "    for images, targets in loop:\n",
    "        # Check for empty batch from collate_fn\n",
    "        if images is None or targets is None:\n",
    "            print(\"Skipping empty or problematic batch.\")\n",
    "            continue\n",
    "            \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Skip batches that became empty after filtering (e.t., all images had 0 valid instances)\n",
    "        if len(images) == 0:\n",
    "            print(\"Skipping batch with no valid instances after processing.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            # Handle potential NaN losses\n",
    "            if torch.isnan(losses):\n",
    "                print(\"Warning: NaN loss detected. Skipping batch.\")\n",
    "                continue\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += losses.item()\n",
    "            batches_processed += 1\n",
    "            loop.set_postfix(loss=losses.item())\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during training step: {e}. Skipping batch.\")\n",
    "            \n",
    "    return total_loss / batches_processed if batches_processed > 0 else 0.0\n",
    "\n",
    "# --- NEW: Evaluation Function ---\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.train() # <-- FIX: Set to train() to get loss dict, but no_grad() will stop updates\n",
    "    total_loss = 0\n",
    "    batches_processed = 0\n",
    "    loop = tqdm(data_loader, leave=True, desc=\"Evaluating\")\n",
    "    \n",
    "    with torch.no_grad(): # Don't calculate gradients\n",
    "        for images, targets in loop:\n",
    "            # Check for empty batch from collate_fn\n",
    "            if images is None or targets is None:\n",
    "                print(\"Skipping empty or problematic batch during evaluation.\")\n",
    "                continue\n",
    "            \n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            if len(images) == 0:\n",
    "                print(\"Skipping batch with no valid instances during evaluation.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # During evaluation, the model still needs targets to calculate loss\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                \n",
    "                if not torch.isnan(losses):\n",
    "                    total_loss += losses.item()\n",
    "                    batches_processed += 1\n",
    "                \n",
    "                loop.set_postfix(val_loss=losses.item())\n",
    "            except Exception as e:\n",
    "                print(f\"Error during evaluation step: {e}. Skipping batch.\")\n",
    "\n",
    "    return total_loss / batches_processed if batches_processed > 0 else 0.0\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Setup Datasets and DataLoaders\n",
    "    print(\"Setting up datasets...\")\n",
    "    dataset_train = NucleiDataset(TRAIN_IMG_DIR, transforms=get_transforms(is_train=True))\n",
    "    dataset_val = NucleiDataset(VAL_IMG_DIR, transforms=get_transforms(is_train=False))\n",
    "\n",
    "    # --- KEY FIX (DataLoader) ---\n",
    "    # Set num_workers=0 to avoid multiprocessing hangs on macOS\n",
    "    train_loader = DataLoader(\n",
    "        dataset_train, batch_size=BATCH_SIZE, shuffle=True, \n",
    "        num_workers=0, collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=False, \n",
    "        num_workers=0, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # 2. Initialize Model, Optimizer\n",
    "    print(\"Initializing model...\")\n",
    "    model = get_model(NUM_CLASSES)\n",
    "    model.to(DEVICE) # Move model to CPU\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    # # 3. Training\n",
    "    # print(f\"Starting training for {NUM_EPOCHS} epochs on CPU. This will take a while...\")\n",
    "    \n",
    "    # # --- NEW: Variables to track the best model ---\n",
    "    # best_val_loss = float('inf')\n",
    "    best_model_path = \"nuclei_maskrcnn_model_cpu_BEST.pth\"\n",
    "\n",
    "    # for epoch in range(NUM_EPOCHS):\n",
    "    #     # --- Train for one epoch ---\n",
    "    #     avg_train_loss = train_one_epoch(model, optimizer, train_loader, DEVICE)\n",
    "        \n",
    "    #     # --- Evaluate on the validation set ---\n",
    "    #     avg_val_loss = evaluate(model, val_loader, DEVICE)\n",
    "        \n",
    "    #     print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    #     # --- NEW: Checkpoint logic ---\n",
    "    #     # Save the model *only if* the validation loss improved\n",
    "    #     if avg_val_loss < best_val_loss and avg_val_loss > 0:\n",
    "    #         best_val_loss = avg_val_loss\n",
    "    #         torch.save(model.state_dict(), best_model_path)\n",
    "    #         print(f\"New best model saved with validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # # 4. Prediction/Inference\n",
    "    # print(\"--- Training finished ---\")\n",
    "    print(f\"Loading best model from {best_model_path} for prediction...\")\n",
    "    \n",
    "    # --- NEW: Load the best model's weights ---\n",
    "    # Ensure model is created before loading weights\n",
    "    model = get_model(NUM_CLASSES) \n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval() # Set to evaluation mode\n",
    "    \n",
    "    test_files = sorted(list(TEST_IMG_DIR.glob(\"*.tif\")))\n",
    "    \n",
    "    # --- NEW: Parameter sweep loop ---\n",
    "    PREDICTION_DIR = Path(\"combinations-maskrcnn-final\")\n",
    "    PREDICTION_DIR.mkdir(exist_ok=True) # Create the folder if it doesn't exist\n",
    "\n",
    "    # --- DEFINE YOUR PARAMETERS TO TEST HERE ---\n",
    "    confidence_threshold_list = [0.1, 0.25, 0.4, 0.5]\n",
    "    mask_threshold_list = [0.4, 0.5]\n",
    "    \n",
    "    print(f\"Starting prediction sweep... saving results to {PREDICTION_DIR}\")\n",
    "\n",
    "    for confidence_threshold in confidence_threshold_list:\n",
    "        for mask_threshold in mask_threshold_list:\n",
    "            \n",
    "            print(f\"--- Running prediction for conf_thresh={confidence_threshold}, mask_thresh={mask_threshold} ---\")\n",
    "            results = []\n",
    "            \n",
    "            for img_path in tqdm(test_files):\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                # Apply the simple tensor conversion and normalization\n",
    "                img_tensor = T.ToImage()(img)\n",
    "                img_tensor = T.ToDtype(torch.float32, scale=True)(img_tensor)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    prediction = model([img_tensor.to(DEVICE)])[0]\n",
    "                    \n",
    "                # Initialize one instance mask per class\n",
    "                instance_masks = {class_name: np.zeros((img.height, img.width), dtype=np.int32) for class_name in CLASS_MAP.keys()}\n",
    "                instance_counters = {class_name: 1 for class_name in CLASS_MAP.keys()}\n",
    "\n",
    "                # Filter predictions by score\n",
    "                scores = prediction['scores'].cpu().numpy()\n",
    "                high_conf_indices = np.where(scores > confidence_threshold)[0]\n",
    "\n",
    "                for i in high_conf_indices:\n",
    "                    label_id = prediction['labels'][i].item()\n",
    "                    if label_id not in INV_CLASS_MAP:\n",
    "                        continue\n",
    "                    \n",
    "                    class_name = INV_CLASS_MAP[label_id]\n",
    "                    \n",
    "                    # Get the binary mask by thresholding the soft mask\n",
    "                    mask = prediction['masks'][i, 0].cpu().numpy()\n",
    "                    binary_mask = (mask > mask_threshold).astype(np.uint8)\n",
    "                    \n",
    "                    # Add instance to the correct class mask with a unique ID\n",
    "                    instance_id = instance_counters[class_name]\n",
    "                    # Ensure no overlap: only assign ID where mask is 1 AND current pixel is 0\n",
    "                    instance_masks[class_name][(binary_mask == 1) & (instance_masks[class_name] == 0)] = instance_id\n",
    "                    instance_counters[class_name] += 1\n",
    "                    \n",
    "                # RLE encode each class mask\n",
    "                rle_results = {\n",
    "                    \"image_id\": img_path.stem,\n",
    "                    \"Epithelial\": rle_encode_instance_mask(instance_masks[\"Epithelial\"]),\n",
    "                    \"Lymphocyte\": rle_encode_instance_mask(instance_masks[\"Lymphocyte\"]),\n",
    "                    \"Macrophage\": rle_encode_instance_mask(instance_masks[\"Macrophage\"]),\n",
    "                    \"Neutrophil\": rle_encode_instance_mask(instance_masks[\"Neutrophil\"])\n",
    "                }\n",
    "                results.append(rle_results)\n",
    "\n",
    "            # 5. Create Submission CSV\n",
    "            csv_filename = PREDICTION_DIR / f\"submission_conf{confidence_threshold}_mask{mask_threshold}.csv\"\n",
    "            submission_df = pd.DataFrame(results, columns=[\"image_id\", \"Epithelial\", \"Lymphocyte\", \"Neutrophil\", \"Macrophage\"])\n",
    "            submission_df.to_csv(csv_filename, index=False)\n",
    "            \n",
    "            print(f\"Saved {csv_filename}\")\n",
    "    \n",
    "    print(\"--- Prediction sweep finished! ---\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
