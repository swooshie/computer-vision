{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6520cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Setting up datasets...\n",
      "Filtering dataset in kaggle-data/train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209/209 [00:02<00:00, 70.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 209 images with valid annotations out of 209 total.\n",
      "Filtering dataset in kaggle-data/val...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 83.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45 images with valid annotations out of 45 total.\n",
      "Initializing model...\n",
      "Starting training for 30 epochs on CPU. This will take a while...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [25:38<00:00, 29.03s/it, loss=1.61]\n",
      "Evaluating: 100%|██████████| 45/45 [01:40<00:00,  2.23s/it, val_loss=1.93] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 2.1529, Val Loss: 1.9273\n",
      "New best model saved with validation loss: 1.9273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [23:41<00:00, 26.81s/it, loss=1.46]\n",
      "Evaluating: 100%|██████████| 45/45 [01:43<00:00,  2.30s/it, val_loss=1.38] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Train Loss: 1.6953, Val Loss: 1.4998\n",
      "New best model saved with validation loss: 1.4998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [23:38<00:00, 26.76s/it, loss=0.641]\n",
      "Evaluating: 100%|██████████| 45/45 [01:43<00:00,  2.31s/it, val_loss=1.34] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Train Loss: 1.5547, Val Loss: 1.3959\n",
      "New best model saved with validation loss: 1.3959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [23:30<00:00, 26.61s/it, loss=1.1]  \n",
      "Evaluating: 100%|██████████| 45/45 [01:34<00:00,  2.10s/it, val_loss=1.4]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Train Loss: 1.5038, Val Loss: 1.3074\n",
      "New best model saved with validation loss: 1.3074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [21:46<00:00, 24.65s/it, loss=1.62] \n",
      "Evaluating: 100%|██████████| 45/45 [01:44<00:00,  2.32s/it, val_loss=1.44] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Train Loss: 1.4407, Val Loss: 1.2724\n",
      "New best model saved with validation loss: 1.2724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [21:58<00:00, 24.87s/it, loss=1.5]  \n",
      "Evaluating: 100%|██████████| 45/45 [01:39<00:00,  2.21s/it, val_loss=1.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Train Loss: 1.3756, Val Loss: 1.3297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [21:19<00:00, 24.15s/it, loss=2.17] \n",
      "Evaluating: 100%|██████████| 45/45 [01:38<00:00,  2.20s/it, val_loss=1.11] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Train Loss: 1.3536, Val Loss: 1.0729\n",
      "New best model saved with validation loss: 1.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [21:35<00:00, 24.45s/it, loss=1.62]\n",
      "Evaluating: 100%|██████████| 45/45 [01:32<00:00,  2.05s/it, val_loss=1.22] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Train Loss: 1.3425, Val Loss: 1.1219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [20:57<00:00, 23.73s/it, loss=1.7] \n",
      "Evaluating: 100%|██████████| 45/45 [01:39<00:00,  2.21s/it, val_loss=1.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Train Loss: 1.3255, Val Loss: 1.1730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [22:50<00:00, 25.86s/it, loss=0.723]\n",
      "Evaluating: 100%|██████████| 45/45 [01:39<00:00,  2.20s/it, val_loss=1.13] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Train Loss: 1.2828, Val Loss: 1.1089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [22:42<00:00, 25.72s/it, loss=0.782]\n",
      "Evaluating: 100%|██████████| 45/45 [01:41<00:00,  2.25s/it, val_loss=0.993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Train Loss: 1.2322, Val Loss: 0.9977\n",
      "New best model saved with validation loss: 0.9977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [23:28<00:00, 26.57s/it, loss=1.22] \n",
      "Evaluating: 100%|██████████| 45/45 [01:40<00:00,  2.23s/it, val_loss=1.14] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Train Loss: 1.2727, Val Loss: 1.0892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [22:48<00:00, 25.82s/it, loss=1.32] \n",
      "Evaluating: 100%|██████████| 45/45 [01:41<00:00,  2.26s/it, val_loss=1.18] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Train Loss: 1.2478, Val Loss: 1.0795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [23:59<00:00, 27.17s/it, loss=1.31] \n",
      "Evaluating: 100%|██████████| 45/45 [01:44<00:00,  2.33s/it, val_loss=1.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Train Loss: 1.3286, Val Loss: 1.0364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [21:56<00:00, 24.83s/it, loss=1.05] \n",
      "Evaluating: 100%|██████████| 45/45 [01:44<00:00,  2.32s/it, val_loss=1.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Train Loss: 1.2189, Val Loss: 0.9961\n",
      "New best model saved with validation loss: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [24:22<00:00, 27.59s/it, loss=0.535]\n",
      "Evaluating: 100%|██████████| 45/45 [02:19<00:00,  3.10s/it, val_loss=1.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30, Train Loss: 1.2088, Val Loss: 1.0647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [29:16<00:00, 33.14s/it, loss=1.73] \n",
      "Evaluating:  87%|████████▋ | 39/45 [01:28<00:10,  1.76s/it, val_loss=0.811]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as T\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set main data directory\n",
    "DATA_DIR = Path(\"kaggle-data\")\n",
    "TRAIN_IMG_DIR = DATA_DIR / \"train\"\n",
    "VAL_IMG_DIR = DATA_DIR / \"val\"\n",
    "TEST_IMG_DIR = DATA_DIR / \"test_final\"\n",
    "\n",
    "# Model parameters\n",
    "NUM_CLASSES = 5 # 4 classes + 1 background\n",
    "BATCH_SIZE = 4 # Keep this low for CPU training, 1 or 2 is good.\n",
    "NUM_EPOCHS = 30 # A more realistic number for starting to see results.\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "# Class mapping\n",
    "CLASS_MAP = {\n",
    "    \"Epithelial\": 1,\n",
    "    \"Lymphocyte\": 2,\n",
    "    \"Macrophage\": 3,\n",
    "    \"Neutrophil\": 4,\n",
    "}\n",
    "# Create an inverse map for prediction\n",
    "INV_CLASS_MAP = {v: k for k, v in CLASS_MAP.items()}\n",
    "\n",
    "# --- KEY FIX (Device) ---\n",
    "# Force CPU to avoid MPS bugs on M-series chips.\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# --- RLE Encoding Function (from problem description) ---\n",
    "def rle_encode_instance_mask(mask: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Convert an instance segmentation mask (H,W) -> RLE triple string.\n",
    "    0 = background, >0 = instance IDs.\n",
    "    \"\"\"\n",
    "    pixels = mask.flatten(order=\"F\").astype(np.int32)\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    rle = []\n",
    "    for i in range(len(runs) - 1):\n",
    "        start = runs[i]\n",
    "        end = runs[i + 1]\n",
    "        length = end - start\n",
    "        val = pixels[start]\n",
    "        if val > 0:\n",
    "            rle.extend([val, start, length])\n",
    "    if not rle:\n",
    "        return \"0\"\n",
    "    return \" \".join(map(str, rle))\n",
    "\n",
    "\n",
    "# --- Custom Dataset Class (Verified Logic) ---\n",
    "class NucleiDataset(Dataset):\n",
    "    def __init__(self, image_dir, transforms=None):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        all_image_files = sorted([f for f in self.image_dir.glob(\"*.tif\")])\n",
    "        all_xml_files = sorted([f for f in self.image_dir.glob(\"*.xml\")])\n",
    "\n",
    "        self.image_files = []\n",
    "        self.xml_files = []\n",
    "        \n",
    "        print(f\"Filtering dataset in {image_dir}...\")\n",
    "        for img_path, xml_path in tqdm(zip(all_image_files, all_xml_files), total=len(all_image_files)):\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # --- KEY FIX (Correct Hierarchy Parsing) ---\n",
    "            # We need to find at least one ANNOTATION that has a class we care about\n",
    "            # AND contains at least one REGION.\n",
    "            has_valid_annotation = False\n",
    "            \n",
    "            # Loop over top-level Annotations\n",
    "            for annotation in root.findall(\"Annotation\"):\n",
    "                # Find the class name for this ANNOTATION\n",
    "                attribute = annotation.find(\"Attributes/Attribute\")\n",
    "                if attribute is None:\n",
    "                    continue\n",
    "                \n",
    "                label_name = attribute.get(\"Name\")\n",
    "                if label_name in CLASS_MAP:\n",
    "                    # This annotation is for a class we care about.\n",
    "                    # Does it contain any regions (instances)?\n",
    "                    if annotation.find(\"Regions/Region\") is not None:\n",
    "                         has_valid_annotation = True\n",
    "                         break # Found a valid class with at least one region, this file is good.\n",
    "            \n",
    "            if has_valid_annotation:\n",
    "                self.image_files.append(img_path)\n",
    "                self.xml_files.append(xml_path)\n",
    "        \n",
    "        print(f\"Found {len(self.image_files)} images with valid annotations out of {len(all_image_files)} total.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        xml_path = self.xml_files[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        width, height = img.size\n",
    "\n",
    "        # Parse XML annotations\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        masks, labels, boxes = [], [], []\n",
    "        \n",
    "        # --- KEY FIX (Correct Hierarchy Parsing) ---\n",
    "        # Loop over top-level Annotations\n",
    "        for annotation in root.findall(\"Annotation\"):\n",
    "            # Find the class name for this ANNOTATION\n",
    "            attribute = annotation.find(\"Attributes/Attribute\")\n",
    "            if attribute is None:\n",
    "                continue\n",
    "                \n",
    "            label_name = attribute.get(\"Name\")\n",
    "            if label_name not in CLASS_MAP:\n",
    "                continue\n",
    "            \n",
    "            # This is a class we care about, e.g., \"Epithelial\"\n",
    "            label_id = CLASS_MAP[label_name]\n",
    "\n",
    "            # Now, find all regions *within this annotation's Regions tag*\n",
    "            for region in annotation.findall(\"Regions/Region\"):\n",
    "                vertices = []\n",
    "                # Try the path from slide1.xml: Region -> Vertices -> Vertex\n",
    "                vertex_nodes = region.findall(\"Vertices/Vertex\")\n",
    "                \n",
    "                if not vertex_nodes:\n",
    "                    # If that fails, try the other common path: Region -> Vertex\n",
    "                    vertex_nodes = region.findall(\"Vertex\")\n",
    "\n",
    "                for vertex in vertex_nodes:\n",
    "                    x = float(vertex.get(\"X\"))\n",
    "                    y = float(vertex.get(\"Y\"))\n",
    "                    vertices.append((x, y))\n",
    "\n",
    "                if not vertices:\n",
    "                    # This region has a class but no vertices, skip it.\n",
    "                    continue\n",
    "\n",
    "                # Create a binary mask for this single instance\n",
    "                instance_mask = Image.new(\"L\", (width, height), 0)\n",
    "                ImageDraw.Draw(instance_mask).polygon(vertices, outline=1, fill=1)\n",
    "                mask_np = np.array(instance_mask)\n",
    "                \n",
    "                # Get bounding box from mask\n",
    "                pos = np.where(mask_np)\n",
    "                if pos[0].size == 0 or pos[1].size == 0:\n",
    "                    continue # Skip empty masks\n",
    "                xmin, xmax = np.min(pos[1]), np.max(pos[1])\n",
    "                ymin, ymax = np.min(pos[0]), np.max(pos[0])\n",
    "                \n",
    "                # Check for valid box area\n",
    "                if xmax > xmin and ymax > ymin:\n",
    "                    masks.append(mask_np)\n",
    "                    labels.append(label_id)\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        if not boxes: # This should now only happen if a \"valid\" file has 0-area polygons\n",
    "            target = {\n",
    "                \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n",
    "                \"labels\": torch.zeros(0, dtype=torch.int64),\n",
    "                \"masks\": torch.zeros((0, height, width), dtype=torch.uint8)\n",
    "            }\n",
    "        else:\n",
    "            # Convert to tensors\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            masks = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n",
    "\n",
    "            target = {\"boxes\": boxes, \"labels\": labels, \"masks\": masks}\n",
    "        \n",
    "        # Apply transforms\n",
    "        img_tensor = T.ToImage()(img) # Convert PIL to tensor\n",
    "        img_tensor = T.ToDtype(torch.float32, scale=True)(img_tensor) # Normalize to [0,1]\n",
    "        \n",
    "        if self.transforms:\n",
    "            # Apply augmentations if any (e.g., RandomHorizontalFlip)\n",
    "            # Note: v2 transforms update target in-place\n",
    "            img_tensor, target = self.transforms(img_tensor, target)\n",
    "\n",
    "        return img_tensor, target\n",
    "\n",
    "# --- Data Augmentation ---\n",
    "def get_transforms(is_train):\n",
    "    transforms = []\n",
    "    # ToImage() and ToDtype() are now handled in __getitem__\n",
    "    if is_train:\n",
    "        # Adds random horizontal flipping for augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    \n",
    "    # --- FIX: Return None if transforms list is empty ---\n",
    "    if not transforms:\n",
    "        return None\n",
    "    \n",
    "    return T.Compose(transforms)\n",
    "\n",
    "# --- Model Definition ---\n",
    "def get_model(num_classes):\n",
    "    # Load a pre-trained instance segmentation model\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # Replace the box predictor\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Replace the mask predictor\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- Utility for DataLoader ---\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    loop = tqdm(data_loader, leave=True)\n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, targets in loop:\n",
    "        # Filter out any 'None' targets that might result from empty images\n",
    "        # This is a safety check\n",
    "        valid_batch = [(img, tgt) for img, tgt in zip(images, targets) if tgt is not None]\n",
    "        if not valid_batch:\n",
    "            print(\"Skipping empty batch\")\n",
    "            continue\n",
    "            \n",
    "        images, targets = zip(*valid_batch)\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Skip batches that became empty after filtering (e.t., all images had 0 valid instances)\n",
    "        if len(images) == 0:\n",
    "            print(\"Skipping batch with no valid instances after processing.\")\n",
    "            continue\n",
    "            \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Handle potential NaN losses\n",
    "        if torch.isnan(losses):\n",
    "            print(\"Warning: NaN loss detected. Skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        loop.set_postfix(loss=losses.item())\n",
    "        \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# --- NEW: Evaluation Function ---\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.train() # <-- FIX: Set to train() to get loss dict, but no_grad() will stop updates\n",
    "    total_loss = 0\n",
    "    loop = tqdm(data_loader, leave=True, desc=\"Evaluating\")\n",
    "    \n",
    "    with torch.no_grad(): # Don't calculate gradients\n",
    "        for images, targets in loop:\n",
    "            valid_batch = [(img, tgt) for img, tgt in zip(images, targets) if tgt is not None]\n",
    "            if not valid_batch:\n",
    "                continue\n",
    "            \n",
    "            images, targets = zip(*valid_batch)\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            if len(images) == 0:\n",
    "                continue\n",
    "\n",
    "            # During evaluation, the model still needs targets to calculate loss\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            if not torch.isnan(losses):\n",
    "                total_loss += losses.item()\n",
    "            \n",
    "            loop.set_postfix(val_loss=losses.item())\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Setup Datasets and DataLoaders\n",
    "    print(\"Setting up datasets...\")\n",
    "    dataset_train = NucleiDataset(TRAIN_IMG_DIR, transforms=get_transforms(is_train=True))\n",
    "    dataset_val = NucleiDataset(VAL_IMG_DIR, transforms=get_transforms(is_train=False))\n",
    "\n",
    "    # --- KEY FIX (DataLoader) ---\n",
    "    # Set num_workers=0 to avoid multiprocessing hangs on macOS\n",
    "    train_loader = DataLoader(\n",
    "        dataset_train, batch_size=BATCH_SIZE, shuffle=True, \n",
    "        num_workers=0, collate_fn=collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=False, \n",
    "        num_workers=0, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # 2. Initialize Model, Optimizer\n",
    "    print(\"Initializing model...\")\n",
    "    model = get_model(NUM_CLASSES)\n",
    "    model.to(DEVICE) # Move model to CPU\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    # 3. Training\n",
    "    print(f\"Starting training for {NUM_EPOCHS} epochs on CPU. This will take a while...\")\n",
    "    \n",
    "    # --- NEW: Variables to track the best model ---\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = \"nuclei_maskrcnn_model_cpu_BEST.pth\"\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # --- Train for one epoch ---\n",
    "        avg_train_loss = train_one_epoch(model, optimizer, train_loader, DEVICE)\n",
    "        \n",
    "        # --- Evaluate on the validation set ---\n",
    "        avg_val_loss = evaluate(model, val_loader, DEVICE)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "        # --- NEW: Checkpoint logic ---\n",
    "        # Save the model *only if* the validation loss improved\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with validation loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # 4. Prediction/Inference\n",
    "    print(\"--- Training finished ---\")\n",
    "    print(f\"Loading best model from {best_model_path} for prediction...\")\n",
    "    \n",
    "    # --- NEW: Load the best model's weights ---\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "    model.eval() # Set to evaluation mode\n",
    "    \n",
    "    test_files = sorted(list(TEST_IMG_DIR.glob(\"*.tif\")))\n",
    "    results = []\n",
    "    \n",
    "    # Get transforms for testing (just normalization)\n",
    "    # --- FIX: Remove this unused and buggy line ---\n",
    "    # test_transforms = get_transforms(is_train=False)\n",
    "\n",
    "    print(\"Starting prediction on the test set...\")\n",
    "    for img_path in tqdm(test_files):\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # Apply the simple tensor conversion and normalization\n",
    "        img_tensor = T.ToImage()(img)\n",
    "        img_tensor = T.ToDtype(torch.float32, scale=True)(img_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model([img_tensor.to(DEVICE)])[0]\n",
    "            \n",
    "        # Initialize one instance mask per class\n",
    "        instance_masks = {class_name: np.zeros((img.height, img.width), dtype=np.int32) for class_name in CLASS_MAP.keys()}\n",
    "        instance_counters = {class_name: 1 for class_name in CLASS_MAP.keys()}\n",
    "\n",
    "        # Filter predictions by score and process them\n",
    "        confidence_threshold = 0.5\n",
    "        scores = prediction['scores'].cpu().numpy()\n",
    "        high_conf_indices = np.where(scores > confidence_threshold)[0]\n",
    "\n",
    "        for i in high_conf_indices:\n",
    "            label_id = prediction['labels'][i].item()\n",
    "            if label_id not in INV_CLASS_MAP:\n",
    "                continue\n",
    "            \n",
    "            class_name = INV_CLASS_MAP[label_id]\n",
    "            \n",
    "            # Get the binary mask by thresholding the soft mask\n",
    "            mask = prediction['masks'][i, 0].cpu().numpy()\n",
    "            binary_mask = (mask > 0.5).astype(np.uint8)\n",
    "            \n",
    "            # Add instance to the correct class mask with a unique ID\n",
    "            instance_id = instance_counters[class_name]\n",
    "            # Ensure no overlap: only assign ID where mask is 1 AND current pixel is 0\n",
    "            instance_masks[class_name][(binary_mask == 1) & (instance_masks[class_name] == 0)] = instance_id\n",
    "            instance_counters[class_name] += 1\n",
    "            \n",
    "        # RLE encode each class mask\n",
    "        rle_results = {\n",
    "            \"image_id\": img_path.stem,\n",
    "            \"Epithelial\": rle_encode_instance_mask(instance_masks[\"Epithelial\"]),\n",
    "            \"Lymphocyte\": rle_encode_instance_mask(instance_masks[\"Lymphocyte\"]),\n",
    "            \"Macrophage\": rle_encode_instance_mask(instance_masks[\"Macrophage\"]),\n",
    "            \"Neutrophil\": rle_encode_instance_mask(instance_masks[\"Neutrophil\"])\n",
    "        }\n",
    "        results.append(rle_results)\n",
    "\n",
    "    # 5. Create Submission CSV\n",
    "    print(\"Creating submission file...\")\n",
    "    submission_df = pd.DataFrame(results, columns=[\"image_id\", \"Epithelial\", \"Lymphocyte\", \"Neutrophil\", \"Macrophage\"])\n",
    "    submission_df.to_csv(\"submission_gpt2.csv\", index=False)\n",
    "    \n",
    "    print(\"submission.csv created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d630252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from nuclei_maskrcnn_model_cpu_BEST.pth for prediction...\n",
      "Starting prediction on the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:02<01:39,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3542218  0.3094444  0.2953994  0.26057822 0.25756732 0.25456735\n",
      " 0.24357812 0.23596412 0.21294756 0.20934927 0.20243208 0.18628149\n",
      " 0.17518887 0.17496729 0.17235251 0.1711742  0.17052142 0.16467665\n",
      " 0.16310106 0.14735453 0.1451866  0.13717528 0.12895611 0.12654467\n",
      " 0.12573816 0.12520695 0.12473088 0.12405452 0.12377945 0.12358025\n",
      " 0.12343706 0.11802054 0.11286068 0.10821794 0.10610596 0.10391385\n",
      " 0.09737283 0.09650794 0.09600981 0.09500112 0.0914961  0.0913388\n",
      " 0.09081868 0.08822686 0.08699933 0.08644167 0.0856558  0.08548825\n",
      " 0.08539207 0.08523829 0.08378797 0.08303632 0.08275808 0.08264152\n",
      " 0.08226836 0.07933556 0.07885344 0.07734136 0.07661338 0.07502476\n",
      " 0.07282703 0.07195829 0.07154873 0.06873851 0.06854381 0.06807977\n",
      " 0.0668014  0.06679291 0.06579588 0.06498147 0.06478667 0.06407136\n",
      " 0.06369562 0.0636515  0.06335458 0.06254822 0.06254367 0.06210437\n",
      " 0.06149235 0.06096388 0.05958416 0.05733868 0.05703201 0.05657746\n",
      " 0.05642383 0.05620978 0.05618202 0.05609224 0.05560296 0.05529748\n",
      " 0.05507309 0.0547465  0.05419016 0.05350683 0.05318531 0.05291389\n",
      " 0.05275588 0.05239925 0.05196439 0.05176724]\n",
      "[0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:56<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission file...\n",
      "submission.csv created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading best model from {best_model_path} for prediction...\")\n",
    "    \n",
    "# --- NEW: Load the best model's weights ---\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "test_files = sorted(list(TEST_IMG_DIR.glob(\"*.tif\")))\n",
    "results = []\n",
    "\n",
    "# Get transforms for testing (just normalization)\n",
    "# --- FIX: Remove this unused and buggy line ---\n",
    "# test_transforms = get_transforms(is_train=False)\n",
    "cnt = 1\n",
    "print(\"Starting prediction on the test set...\")\n",
    "for img_path in tqdm(test_files):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    # Apply the simple tensor conversion and normalization\n",
    "    img_tensor = T.ToImage()(img)\n",
    "    img_tensor = T.ToDtype(torch.float32, scale=True)(img_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = model([img_tensor.to(DEVICE)])[0]\n",
    "        \n",
    "    # Initialize one instance mask per class\n",
    "    instance_masks = {class_name: np.zeros((img.height, img.width), dtype=np.int32) for class_name in CLASS_MAP.keys()}\n",
    "    instance_counters = {class_name: 1 for class_name in CLASS_MAP.keys()}\n",
    "\n",
    "    # Filter predictions by score and process them\n",
    "    confidence_threshold = 0.3\n",
    "    scores = prediction['scores'].cpu().numpy()\n",
    "    high_conf_indices = np.where(scores > confidence_threshold)[0]\n",
    "\n",
    "    if cnt == 1:\n",
    "        print(scores)\n",
    "        print(high_conf_indices)\n",
    "        cnt += 1\n",
    "\n",
    "    for i in high_conf_indices:\n",
    "        label_id = prediction['labels'][i].item()\n",
    "        if label_id not in INV_CLASS_MAP:\n",
    "            continue\n",
    "        \n",
    "        class_name = INV_CLASS_MAP[label_id]\n",
    "        \n",
    "        # Get the binary mask by thresholding the soft mask\n",
    "        mask = prediction['masks'][i, 0].cpu().numpy()\n",
    "        binary_mask = (mask > 0.5).astype(np.uint8)\n",
    "        \n",
    "        # Add instance to the correct class mask with a unique ID\n",
    "        instance_id = instance_counters[class_name]\n",
    "        # Ensure no overlap: only assign ID where mask is 1 AND current pixel is 0\n",
    "        instance_masks[class_name][(binary_mask == 1) & (instance_masks[class_name] == 0)] = instance_id\n",
    "        instance_counters[class_name] += 1\n",
    "        \n",
    "    # RLE encode each class mask\n",
    "    rle_results = {\n",
    "        \"image_id\": img_path.stem,\n",
    "        \"Epithelial\": rle_encode_instance_mask(instance_masks[\"Epithelial\"]),\n",
    "        \"Lymphocyte\": rle_encode_instance_mask(instance_masks[\"Lymphocyte\"]),\n",
    "        \"Macrophage\": rle_encode_instance_mask(instance_masks[\"Macrophage\"]),\n",
    "        \"Neutrophil\": rle_encode_instance_mask(instance_masks[\"Neutrophil\"])\n",
    "    }\n",
    "    results.append(rle_results)\n",
    "\n",
    "# 5. Create Submission CSV\n",
    "print(\"Creating submission file...\")\n",
    "submission_df = pd.DataFrame(results, columns=[\"image_id\", \"Epithelial\", \"Lymphocyte\", \"Neutrophil\", \"Macrophage\"])\n",
    "submission_df.to_csv(\"submission_gpt2.csv\", index=False)\n",
    "\n",
    "print(\"submission.csv created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
