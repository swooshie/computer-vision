{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27d1dffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Phase 1: Training the HoVer-Net Model (with MPS)\n",
      "‚úÖ Using Apple MPS for GPU acceleration.\n",
      "\n",
      "--- Step 1: Loading Datasets ---\n",
      "Dataset for train found 209 images.\n",
      "Dataset for val found 45 images.\n",
      "DataLoaders created.\n",
      "\n",
      "--- Step 2: Initializing Model ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "HoverNet.__init__() got an unexpected keyword argument 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 383\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    382\u001b[39m     logging.basicConfig(level=logging.INFO)\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 336\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Step 2: Initializing Model ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    335\u001b[39m \u001b[38;5;66;03m# This single line gets us the complex HoVer-Net model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m model = \u001b[43mhovernet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mHoverNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_classes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_encoder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimagenet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use a pre-trained backbone\u001b[39;49;00m\n\u001b[32m    339\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# This single line gets us the complex 3-part loss function\u001b[39;00m\n\u001b[32m    342\u001b[39m criterion = losses.mss_loss(num_classes=CONFIG[\u001b[33m\"\u001b[39m\u001b[33mnum_classes\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[31mTypeError\u001b[39m: HoverNet.__init__() got an unexpected keyword argument 'num_classes'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tifffile import imread\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from scipy import ndimage as ndi  # <-- IMPORTING SCIPY FOR HV MAPS\n",
    "\n",
    "# --- HoVer-Net Library Imports ---\n",
    "from cellseg_models_pytorch import models\n",
    "from cellseg_models_pytorch.models import hovernet\n",
    "from cellseg_models_pytorch import losses\n",
    "# *** ALL BROKEN UTILS IMPORTS ARE REMOVED ***\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"base_dir\": \"kaggle-data\",\n",
    "    \"train_dir\": \"train\",\n",
    "    \"val_dir\": \"val\",\n",
    "    \"model_dir\": os.path.join(\"kaggle-data\", \"hovernet_model_mps\"),\n",
    "    \"model_name\": \"hovernet_best.pth\",\n",
    "    \"class_names\": [\"Epithelial\", \"Lymphocyte\", \"Macrophage\", \"Neutrophil\"],\n",
    "    # Training parameters\n",
    "    \"n_epochs\": 100,\n",
    "    \"batch_size\": 1, # Start with 1. Increase to 2 or 4 if your M3 Pro can handle it.\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_classes\": 5 # 4 classes + 1 background\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# NEW HV-MAP FUNCTION (REPLACES 'prep_data')\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def compute_hv_maps(inst_map: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the Horizontal/Vertical (HV) maps for HoVer-Net.\n",
    "    This is the manual implementation of the library's missing 'prep_data'.\n",
    "    \n",
    "    Args:\n",
    "        inst_map (np.ndarray): The instance segmentation map (H, W).\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: The HV map (H, W, 2)\n",
    "    \"\"\"\n",
    "    H, W = inst_map.shape\n",
    "    \n",
    "    # 1. Create X and Y coordinate grids\n",
    "    x_coords = np.arange(W, dtype=np.float32)\n",
    "    y_coords = np.arange(H, dtype=np.float32)\n",
    "    xx, yy = np.meshgrid(x_coords, y_coords) # (H, W)\n",
    "    \n",
    "    # 2. Initialize output maps\n",
    "    hv_map = np.zeros((H, W, 2), dtype=np.float32)\n",
    "    \n",
    "    # 3. Find centers of mass for each nucleus\n",
    "    # This gives a list of (y, x) tuples\n",
    "    try:\n",
    "        centers_of_mass = ndi.center_of_mass(\n",
    "            inst_map, inst_map, range(1, np.max(inst_map) + 1)\n",
    "        )\n",
    "    except Exception:\n",
    "        # Handle rare case of empty mask\n",
    "        return hv_map\n",
    "\n",
    "    # Iterate over each nucleus found\n",
    "    for inst_id in range(1, np.max(inst_map) + 1):\n",
    "        if inst_id - 1 < len(centers_of_mass):\n",
    "            center_y, center_x = centers_of_mass[inst_id - 1]\n",
    "            \n",
    "            # 4. Get all pixels for this nucleus\n",
    "            mask = (inst_map == inst_id)\n",
    "            \n",
    "            # 5. Compute vector from each pixel to the center\n",
    "            # Vector = Center - Pixel_Coord\n",
    "            hv_map_x = center_x - xx[mask]\n",
    "            hv_map_y = center_y - yy[mask]\n",
    "            \n",
    "            # 6. Normalize the vectors (a key part of HoVer-Net)\n",
    "            mag_x = np.abs(hv_map_x).max()\n",
    "            mag_y = np.abs(hv_map_y).max()\n",
    "            \n",
    "            if mag_x > 0:\n",
    "                hv_map_x /= mag_x\n",
    "            if mag_y > 0:\n",
    "                hv_map_y /= mag_y\n",
    "            \n",
    "            # 7. Assign to the final map\n",
    "            hv_map[mask, 0] = hv_map_y\n",
    "            hv_map[mask, 1] = hv_map_x\n",
    "                \n",
    "    return hv_map\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DATA LOADING FUNCTIONS (Unchanged)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def parse_xml_for_hovernet(xml_path: str) -> (list, list):\n",
    "    \"\"\"\n",
    "    Parses an XML file to get polygons and their associated class index.\n",
    "    \"\"\"\n",
    "    polygons = []\n",
    "    class_ids = []\n",
    "    class_map = {name: i + 1 for i, name in enumerate(CONFIG[\"class_names\"])} # 1-indexed classes\n",
    "    \n",
    "    if not os.path.exists(xml_path):\n",
    "        return polygons, class_ids\n",
    "        \n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    for annotation_node in root.findall('.//Annotation'):\n",
    "        name_attrib = None\n",
    "        if 'Name' in annotation_node.attrib and annotation_node.attrib['Name'] in class_map:\n",
    "            name_attrib = annotation_node.attrib['Name']\n",
    "        else:\n",
    "            attrib_node = annotation_node.find('.//Attribute')\n",
    "            if attrib_node is not None and 'Name' in attrib_node.attrib:\n",
    "                if attrib_node.attrib['Name'] in class_map:\n",
    "                    name_attrib = attrib_node.attrib['Name']\n",
    "\n",
    "        if name_attrib:\n",
    "            class_id = class_map[name_attrib]\n",
    "            for region_node in annotation_node.findall('.//Region'):\n",
    "                vertices = []\n",
    "                for vertex_node in region_node.findall('.//Vertex'):\n",
    "                    x = float(vertex_node.get('X'))\n",
    "                    y = float(vertex_node.get('Y'))\n",
    "                    vertices.append([x, y])\n",
    "                if vertices:\n",
    "                    polygons.append(np.array(vertices, dtype=np.int32))\n",
    "                    class_ids.append(class_id)\n",
    "                    \n",
    "    return polygons, class_ids\n",
    "\n",
    "def create_hovernet_maps(polygons: list, class_ids: list, height: int, width: int) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Creates the two target maps for HoVer-Net:\n",
    "    1. Instance Map: Each nucleus has a unique ID (1, 2, 3...)\n",
    "    2. Type Map: Each nucleus pixel is colored by its class ID (1, 2, 3, or 4)\n",
    "    \"\"\"\n",
    "    inst_map = np.zeros((height, width), dtype=np.uint16)\n",
    "    type_map = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    for i, (polygon, class_id) in enumerate(zip(polygons, class_ids)):\n",
    "        instance_id = i + 1\n",
    "        cv2.fillPoly(inst_map, [polygon], instance_id)\n",
    "        cv2.fillPoly(type_map, [polygon], class_id)\n",
    "        \n",
    "    return inst_map, type_map\n",
    "\n",
    "class NucleiDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset.\n",
    "    This loads one image and its masks at a time, processes them,\n",
    "    and returns them as Tensors. This is memory-safe.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir):\n",
    "        self.image_files = sorted(glob(os.path.join(data_dir, '*.tif')))\n",
    "        print(f\"Dataset for {os.path.basename(data_dir)} found {len(self.image_files)} images.\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        xml_path = os.path.join(os.path.dirname(img_path), f\"{base_name}.xml\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Load Image\n",
    "            image = imread(img_path)\n",
    "            if image.ndim == 3 and image.shape[-1] == 4:\n",
    "                image = image[..., :3] # Remove alpha\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            # 2. Load Annotations\n",
    "            polygons, class_ids = parse_xml_for_hovernet(xml_path)\n",
    "            if not polygons:\n",
    "                # If no polygons, return None to be skipped by collate_fn\n",
    "                return None\n",
    "\n",
    "            # 3. Create Intermediate Target Maps\n",
    "            inst_map, type_map = create_hovernet_maps(polygons, class_ids, height, width)\n",
    "\n",
    "            # *** THIS IS THE KEY CHANGE ***\n",
    "            # 4. Use our *own* function to compute the HV map\n",
    "            hv_map = compute_hv_maps(inst_map)\n",
    "            \n",
    "            # 5. Create the Nuclei Pixel (NP) map (binary segmentation)\n",
    "            np_map = (inst_map > 0).astype(np.uint8)\n",
    "\n",
    "            # 6. Normalize and Convert to Tensors\n",
    "            image = (image - image.min()) / (image.max() - image.min() + 1e-6) # Simple 0-1 norm\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() # HWC -> CHW\n",
    "            \n",
    "            # Create the correct target dictionary for mss_loss\n",
    "            targets = {\n",
    "                \"np_map\": torch.from_numpy(np_map).long(),   # (H, W)\n",
    "                \"hv_map\": torch.from_numpy(hv_map).float(), # (H, W, 2)\n",
    "                \"type_map\": torch.from_numpy(type_map).long() # (H, W)\n",
    "            }\n",
    "            \n",
    "            return image, targets\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}. Skipping.\")\n",
    "            return None\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    A custom collate function to filter out None values\n",
    "    (from images with no annotations).\n",
    "    \"\"\"\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if not batch:\n",
    "        return None, None\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MANUAL TRAINING FUNCTIONS (Unchanged)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Performs one full training epoch.\n",
    "    \"\"\"\n",
    "    model.train() # Set model to training mode\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # Use tqdm for a nice progress bar\n",
    "    for images, targets in tqdm(loader, desc=\"Training\"):\n",
    "        # Handle empty batches from collate_fn\n",
    "        if images is None:\n",
    "            continue\n",
    "            \n",
    "        # Move data to the MPS device\n",
    "        images = images.to(device)\n",
    "        targets = {k: v.to(device) for k, v in targets.items()}\n",
    "        \n",
    "        # 1. Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # 3. Calculate loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # 4. Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Performs one full validation epoch.\n",
    "    \"\"\"\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient calculations\n",
    "        for images, targets in tqdm(loader, desc=\"Validation\"):\n",
    "            # Handle empty batches\n",
    "            if images is None:\n",
    "                continue\n",
    "                \n",
    "            # Move data to MPS device\n",
    "            images = images.to(device)\n",
    "            targets = {k: v.to(device) for k, v in targets.items()}\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MAIN FUNCTION (Unchanged)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training function for HoVer-Net using MPS.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting Phase 1: Training the HoVer-Net Model (with MPS)\")\n",
    "    os.makedirs(CONFIG[\"model_dir\"], exist_ok=True)\n",
    "\n",
    "    # 1. Set up Device (MPS for Apple Silicon)\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"‚úÖ Using Apple MPS for GPU acceleration.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"‚ö†Ô∏è MPS not found. Using CPU.\")\n",
    "        \n",
    "    # 2. Create Datasets and DataLoaders\n",
    "    print(\"\\n--- Step 1: Loading Datasets ---\")\n",
    "    train_ds = NucleiDataset(data_dir=os.path.join(CONFIG[\"base_dir\"], CONFIG[\"train_dir\"]))\n",
    "    val_ds = NucleiDataset(data_dir=os.path.join(CONFIG[\"base_dir\"], CONFIG[\"val_dir\"]))\n",
    "\n",
    "    # We use num_workers=0 to avoid multiprocessing issues on Mac\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        num_workers=0 \n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=CONFIG[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        num_workers=0\n",
    "    )\n",
    "    print(\"DataLoaders created.\")\n",
    "\n",
    "    # 3. Initialize Model, Loss, and Optimizer\n",
    "    print(\"\\n--- Step 2: Initializing Model ---\")\n",
    "    \n",
    "    # This single line gets us the complex HoVer-Net model\n",
    "    model = hovernet.HoverNet(\n",
    "        num_classes=CONFIG[\"num_classes\"],\n",
    "        pretrained_encoder=\"imagenet\" # Use a pre-trained backbone\n",
    "    ).to(device)\n",
    "    \n",
    "    # This single line gets us the complex 3-part loss function\n",
    "    criterion = losses.mss_loss(num_classes=CONFIG[\"num_classes\"])\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=CONFIG[\"lr\"])\n",
    "    \n",
    "    print(\"Model, Loss, and Optimizer are ready.\")\n",
    "\n",
    "    # 4. Set up the MANUAL Training Loop\n",
    "    print(\"\\n--- Step 3: Starting Model Training ---\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    save_path = os.path.join(CONFIG[\"model_dir\"], CONFIG[\"model_name\"])\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(CONFIG[\"n_epochs\"]):\n",
    "            print(f\"\\n--- Epoch {epoch+1}/{CONFIG['n_epochs']} ---\")\n",
    "            \n",
    "            # Run one training epoch\n",
    "            train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "            \n",
    "            # Run one validation epoch\n",
    "            val_loss = evaluate(model, val_loader, criterion, device)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # 5. Save the best model\n",
    "            if val_loss < best_val_loss:\n",
    "                print(f\"New best model! Val loss improved from {best_val_loss:.4f} to {val_loss:.4f}.\")\n",
    "                print(f\"Saving to {save_path}\")\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n--- Training interrupted by user. ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- An error occurred during training: {e} ---\")\n",
    "        logging.exception(\"Training error\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Phase 1 Complete. Best model saved to: {save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98786e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tifffile import imread\n",
    "import logging\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- HoVer-Net Library Imports ---\n",
    "from cellseg_models_pytorch import models\n",
    "from cellseg_models_pytorch.post_process import hovernet_post_process\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"base_dir\": \"kaggle-data\",\n",
    "    \"test_dir\": \"test_final\",\n",
    "    \"model_dir\": os.path.join(\"kaggle-data\", \"hovernet_model_mps\"),\n",
    "    \"model_name\": \"hovernet_best.pth\",\n",
    "    \"class_names\": [\"Epithelial\", \"Lymphocyte\", \"Macrophage\", \"Neutrophil\"],\n",
    "    \"num_classes\": 5, # 4 classes + 1 background\n",
    "    \"submission_file\": \"submission_hovernet.csv\"\n",
    "}\n",
    "\n",
    "def rle_encode(img):\n",
    "    \"\"\"\n",
    "    Encodes a binary mask into Run-Length Encoding (RLE) string.\n",
    "    \"\"\"\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main inference function for HoVer-Net using MPS.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting Inference with trained HoVer-Net Model (with MPS)\")\n",
    "\n",
    "    # 1. Set up Device (MPS for Apple Silicon)\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"‚úÖ Using Apple MPS for GPU acceleration.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"‚ö†Ô∏è MPS not found. Using CPU.\")\n",
    "\n",
    "    # 2. Initialize and Load Model\n",
    "    print(\"\\n--- Step 1: Loading Trained Model ---\")\n",
    "    model_path = os.path.join(CONFIG[\"model_dir\"], CONFIG[\"model_name\"])\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"FATAL ERROR: Model file not found at {model_path}\")\n",
    "        print(\"Please run train_hovernet_mps.py first.\")\n",
    "        return\n",
    "\n",
    "    model = models.hovernet(num_classes=CONFIG[\"num_classes\"]).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    print(f\"Model loaded successfully from {model_path}\")\n",
    "\n",
    "    # 3. Find Test Images\n",
    "    test_files = sorted(glob(os.path.join(CONFIG[\"base_dir\"], CONFIG[\"test_dir\"], '*.tif')))\n",
    "    print(f\"Found {len(test_files)} test images.\")\n",
    "    \n",
    "    # 4. Run Inference Loop\n",
    "    print(\"\\n--- Step 2: Running Inference on Test Images ---\")\n",
    "    results = []\n",
    "    class_map = {i + 1: name for i, name in enumerate(CONFIG[\"class_names\"])} # 1-indexed\n",
    "\n",
    "    for img_path in tqdm(test_files, desc=\"Generating predictions\"):\n",
    "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        \n",
    "        try:\n",
    "            # 1. Load and prepare image\n",
    "            image = imread(img_path)\n",
    "            if image.ndim == 3 and image.shape[-1] == 4:\n",
    "                image = image[..., :3]\n",
    "            \n",
    "            # Normalize\n",
    "            image_norm = (image - image.min()) / (image.max() - image.min() + 1e-6)\n",
    "            \n",
    "            # Convert to Tensor, add batch dim, send to MPS\n",
    "            image_tensor = torch.from_numpy(image_norm).permute(2, 0, 1).float()\n",
    "            image_tensor = image_tensor.unsqueeze(0).to(device) # BCHW\n",
    "            \n",
    "            # 2. Run Model\n",
    "            with torch.no_grad():\n",
    "                # The model returns a dictionary of outputs\n",
    "                output = model(image_tensor)\n",
    "                \n",
    "            # Move outputs to CPU and remove batch dim\n",
    "            output = {k: v.cpu().numpy().squeeze() for k, v in output.items()}\n",
    "\n",
    "            # 3. Post-processing\n",
    "            # This is the magic function that converts model output into usable masks\n",
    "            inst_map, type_map = hovernet_post_process(\n",
    "                output,\n",
    "                nms_thresh=0.4,\n",
    "                type_thresh=0.5,\n",
    "                inst_thresh=0.3\n",
    "            )\n",
    "\n",
    "            # 4. Generate RLEs for submission\n",
    "            rle_masks = {\"image_id\": base_name}\n",
    "            for class_id, class_name in class_map.items():\n",
    "                # Create a binary mask for *this class only*\n",
    "                binary_mask = (type_map == class_id).astype(np.uint8)\n",
    "                \n",
    "                # Encode the mask\n",
    "                rle_masks[class_name] = rle_encode(binary_mask)\n",
    "            \n",
    "            results.append(rle_masks)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}. Appending empty RLEs.\")\n",
    "            results.append({\n",
    "                \"image_id\": base_name,\n",
    "                **{name: \"\" for name in CONFIG[\"class_names\"]}\n",
    "            })\n",
    "\n",
    "    # 5. Save Submission File\n",
    "    print(\"\\n--- Step 3: Saving Submission File ---\")\n",
    "    df = pd.DataFrame(\n",
    "        results,\n",
    "        columns=[\"image_id\"] + CONFIG[\"class_names\"]\n",
    "    )\n",
    "    df.to_csv(CONFIG[\"submission_file\"], index=False)\n",
    "    print(f\"‚úÖ Inference complete. Submission file saved to: {CONFIG['submission_file']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
