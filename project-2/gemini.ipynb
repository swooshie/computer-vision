{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf571146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Dict, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# --- RLE Encoding and Decoding Functions (Provided in the Prompt) ---\n",
    "\n",
    "def rle_decode_instance_mask(rle: str, shape: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert RLE triple string back into an instance mask of shape (H, W).\n",
    "    \"\"\"\n",
    "    if not rle or str(rle).strip() in (\"\", \"0\", \"nan\"):\n",
    "        return np.zeros(shape, dtype=np.uint16)\n",
    "    \n",
    "    # Handle potential float/NaN from pandas read\n",
    "    if isinstance(rle, float) and np.isnan(rle):\n",
    "        return np.zeros(shape, dtype=np.uint16)\n",
    "    \n",
    "    s = list(map(int, str(rle).split()))\n",
    "    mask = np.zeros(shape[0]*shape[1], dtype=np.uint16)\n",
    "    \n",
    "    # s is a list of [val, start, length, val, start, length, ...]\n",
    "    for i in range(0, len(s), 3):\n",
    "        val, start, length = s[i], s[i+1], s[i+2]\n",
    "        # RLE uses 1-based indexing\n",
    "        mask[start-1:start-1+length] = val\n",
    "        \n",
    "    # Reshape back to (H, W) using 'F' order (column-major/Fortran order)\n",
    "    return mask.reshape(shape, order=\"F\")\n",
    "\n",
    "def rle_encode_instance_mask(mask: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Convert an instance segmentation mask (H,W) -> RLE triple string.\n",
    "    0 = background, >0 = instance IDs.\n",
    "    \"\"\"\n",
    "    # Flatten using Fortran order (column-major)\n",
    "    pixels = mask.flatten(order=\"F\").astype(np.int32)\n",
    "    # Pad with 0s for run-length detection\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    # Find the indices where the value changes\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "\n",
    "    rle = []\n",
    "    # Iterate through runs to extract the [value, start, length] triples\n",
    "    for i in range(0, len(runs)-1):\n",
    "        start = runs[i]\n",
    "        end = runs[i+1]\n",
    "        length = end - start\n",
    "        val = pixels[start]\n",
    "        # Only encode instances (val > 0)\n",
    "        if val > 0:\n",
    "            rle.extend([val, start, length])\n",
    "\n",
    "    if not rle:\n",
    "        return \"0\" # Return \"0\" if no instances are found\n",
    "\n",
    "    return \" \".join(map(str, rle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e55f520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining image dimensions...\n",
      "Loaded annotations for 209 images.\n",
      "\n",
      "First 3 rows of the processed DataFrame:\n",
      "  image_id  Height  Width                                         Epithelial\n",
      "0   slide1     297    204                                                  0\n",
      "1   slide2     783    915  191 3596 1 191 4378 14 191 5162 15 191 5947 16...\n",
      "2   slide3     231    275                                                  0\n",
      "\n",
      "Example Image: slide1 (Shape: 297x204)\n",
      "  - Epithelial: 0 instances found.\n",
      "  - Lymphocyte: 0 instances found.\n",
      "  - Macrophage: 5 instances found.\n",
      "  - Neutrophil: 0 instances found.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "DATA_DIR = \"kaggle-data\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test_final\")\n",
    "ANNOTATION_FILE = os.path.join(DATA_DIR, \"train_ground_truth.csv\")\n",
    "TARGET_CLASSES = ['Epithelial', 'Lymphocyte', 'Macrophage', 'Neutrophil']\n",
    "\n",
    "# --- Main Data Loading Function ---\n",
    "\n",
    "def load_and_decode_masks(annotation_file: str, train_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the ground truth CSV and adds image dimensions to each row.\n",
    "    \"\"\"\n",
    "    # Read the annotation file\n",
    "    df = pd.read_csv(annotation_file)\n",
    "    \n",
    "    # 1. Determine image shapes\n",
    "    print(\"Determining image dimensions...\")\n",
    "    shapes = {}\n",
    "    for image_id in df['image_id']:\n",
    "        image_path = os.path.join(train_dir, f\"{image_id}.tif\")\n",
    "        try:\n",
    "            # Use PIL to quickly read the TIF metadata (size)\n",
    "            with Image.open(image_path) as img:\n",
    "                # Shape is (Height, Width)\n",
    "                shapes[image_id] = (img.height, img.width)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Image file not found for {image_id}\")\n",
    "            shapes[image_id] = (0, 0) # Placeholder for missing file\n",
    "\n",
    "    # 2. Add H and W columns to the dataframe\n",
    "    df['Height'] = df['image_id'].map(lambda x: shapes.get(x, (0, 0))[0])\n",
    "    df['Width'] = df['image_id'].map(lambda x: shapes.get(x, (0, 0))[1])\n",
    "    \n",
    "    print(f\"Loaded annotations for {len(df)} images.\")\n",
    "    return df\n",
    "\n",
    "# --- Execution ---\n",
    "# Load the dataframe with image sizes\n",
    "train_df = load_and_decode_masks(ANNOTATION_FILE, TRAIN_DIR)\n",
    "print(\"\\nFirst 3 rows of the processed DataFrame:\")\n",
    "print(train_df[['image_id', 'Height', 'Width', 'Epithelial']].head(3))\n",
    "\n",
    "# --- Example of Decoding a Single Image's Masks ---\n",
    "\n",
    "EXAMPLE_ID = train_df['image_id'].iloc[0]\n",
    "example_row = train_df[train_df['image_id'] == EXAMPLE_ID].iloc[0]\n",
    "H, W = example_row['Height'], example_row['Width']\n",
    "\n",
    "print(f\"\\nExample Image: {EXAMPLE_ID} (Shape: {H}x{W})\")\n",
    "\n",
    "# Dictionary to hold the instance mask for each class\n",
    "decoded_masks: Dict[str, np.ndarray] = {}\n",
    "\n",
    "for class_name in TARGET_CLASSES:\n",
    "    rle_string = example_row[class_name]\n",
    "    # Decode the RLE string into a 2D instance mask\n",
    "    mask = rle_decode_instance_mask(rle_string, (H, W))\n",
    "    decoded_masks[class_name] = mask\n",
    "    \n",
    "    num_instances = np.unique(mask[mask > 0]).size\n",
    "    print(f\"  - {class_name}: {num_instances} instances found.\")\n",
    "\n",
    "# --- Visualization (Optional but highly recommended) ---\n",
    "def visualize_masks(image_id: str, image_dir: str, masks: Dict[str, np.ndarray]):\n",
    "    \"\"\"Visualizes the original image and the decoded instance masks.\"\"\"\n",
    "    \n",
    "    image_path = os.path.join(image_dir, f\"{image_id}.tif\")\n",
    "    try:\n",
    "        image = np.array(Image.open(image_path))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Could not load image {image_id} for visualization.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "    \n",
    "    # 1. Original Image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(f'Original H&E ({image_id})')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Assign a unique color to each class for the overlay\n",
    "    class_colors = {\n",
    "        'Epithelial': 'red',\n",
    "        'Lymphocyte': 'yellow',\n",
    "        'Macrophage': 'cyan',\n",
    "        'Neutrophil': 'lime'\n",
    "    }\n",
    "    \n",
    "    # 2. Combined Overlay (all classes)\n",
    "    overlay = np.zeros(masks['Epithelial'].shape + (3,), dtype=np.uint8)\n",
    "    for i, class_name in enumerate(TARGET_CLASSES):\n",
    "        mask = masks[class_name]\n",
    "        # Create a single color layer for this class\n",
    "        color_rgb = np.array(plt.colormaps.get_cmap('hsv')(i / len(TARGET_CLASSES))[:3]) * 255\n",
    "        # Find all instance pixels\n",
    "        class_pixels = mask > 0\n",
    "        # Add color to the overlay\n",
    "        overlay[class_pixels] = color_rgb.astype(np.uint8)\n",
    "\n",
    "    # Blend the image and the mask\n",
    "    # For visualization, we use a simple transparency overlay\n",
    "    blended = image.copy()\n",
    "    alpha = 0.5\n",
    "    blended[overlay > 0] = (blended[overlay > 0] * (1 - alpha) + overlay[overlay > 0] * alpha).astype(np.uint8)\n",
    "    \n",
    "    axes[1].imshow(blended)\n",
    "    axes[1].set_title('Combined Instance Overlay')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # 3-6. Individual Class Masks\n",
    "    for i, class_name in enumerate(TARGET_CLASSES):\n",
    "        ax = axes[i + 1]\n",
    "        mask = masks[class_name]\n",
    "        # Show mask boundaries or color-coded instances\n",
    "        ax.imshow(mask > 0, cmap='gray') \n",
    "        ax.set_title(f'{class_name} ({np.unique(mask[mask>0]).size} Inst.)', color=class_colors.get(class_name, 'white'))\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the visualization for the example image\n",
    "# visualize_masks(EXAMPLE_ID, TRAIN_DIR, decoded_masks) \n",
    "# Uncomment the line above to run the visualization in a Jupyter environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc9d919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import distance_transform_edt\n",
    "from skimage.segmentation import watershed\n",
    "\n",
    "def generate_target_maps(instance_masks: Dict[str, np.ndarray], shape: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates a multi-channel target map for a U-Net model.\n",
    "    Channels: [Semantic_Nuclei_Mask, Dist_Map_Centers, Class_Map]\n",
    "    \"\"\"\n",
    "    H, W = shape\n",
    "    \n",
    "    # 1. Combined Instance Mask (for distance calculation)\n",
    "    # Combine all nuclei into one mask, maintaining unique instance IDs\n",
    "    combined_instance_mask = np.zeros((H, W), dtype=np.uint16)\n",
    "    \n",
    "    # 2. Semantic Nuclei Mask (Channel 0: All nuclei, value 1)\n",
    "    semantic_nuclei_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "    \n",
    "    # 3. Class Map (Channel 2: 0=Bkg, 1=Epithelial, 2=Lymphocyte, 3=Macrophage, 4=Neutrophil)\n",
    "    # The classification targets are crucial for the classification loss.\n",
    "    class_map = np.zeros((H, W), dtype=np.uint8)\n",
    "    \n",
    "    class_to_id = {'Epithelial': 1, 'Lymphocyte': 2, 'Macrophage': 3, 'Neutrophil': 4}\n",
    "    \n",
    "    # Track global instance ID across all classes\n",
    "    current_max_id = 0\n",
    "    \n",
    "    for class_name, class_id in class_to_id.items():\n",
    "        mask = instance_masks.get(class_name, np.zeros((H, W), dtype=np.uint16))\n",
    "        \n",
    "        # Get unique instances for this class\n",
    "        unique_ids = np.unique(mask[mask > 0])\n",
    "        \n",
    "        for inst_id in unique_ids:\n",
    "            instance_region = (mask == inst_id)\n",
    "            \n",
    "            # Update Semantic Mask\n",
    "            semantic_nuclei_mask[instance_region] = 1\n",
    "            \n",
    "            # Update Class Map\n",
    "            class_map[instance_region] = class_id\n",
    "            \n",
    "            # Update Combined Instance Mask with unique global IDs\n",
    "            # This is needed for a single distance map calculation\n",
    "            # We shift the local instance ID to a global unique ID\n",
    "            new_global_id = current_max_id + 1\n",
    "            combined_instance_mask[instance_region] = new_global_id\n",
    "            current_max_id += 1\n",
    "            \n",
    "    # 4. Distance Map to Centers (Channel 1)\n",
    "    # The distance transform is a good way to create targets for separating instances.\n",
    "    dist_map = np.zeros((H, W), dtype=np.float32)\n",
    "    \n",
    "    # Calculate a distance map for each instance and take the maximum distance for that pixel\n",
    "    unique_global_ids = np.unique(combined_instance_mask[combined_instance_mask > 0])\n",
    "    for inst_id in unique_global_ids:\n",
    "        instance_region = (combined_instance_mask == inst_id)\n",
    "        # Calculate distance to the boundary of this instance\n",
    "        dist_to_boundary = distance_transform_edt(instance_region)\n",
    "        # Update the final distance map: dist_map will hold the distance to the center of the instance it belongs to\n",
    "        dist_map[instance_region] = dist_to_boundary[instance_region]\n",
    "        \n",
    "    # Normalize the distance map for better U-Net training stability\n",
    "    max_dist = dist_map.max()\n",
    "    if max_dist > 0:\n",
    "        dist_map /= max_dist\n",
    "        \n",
    "    # Stack the channels: (H, W, 3) -> [Semantic Mask, Normalized Distance Map, Class Map]\n",
    "    # Note: Class Map is often best handled by applying classification loss only on nuclei pixels\n",
    "    target = np.stack([\n",
    "        semantic_nuclei_mask, \n",
    "        dist_map, \n",
    "        class_map\n",
    "    ], axis=-1)\n",
    "    \n",
    "    return target\n",
    "\n",
    "# --- End of Target Generation Utilities ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99a11962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataLoader created. Train size: 188, Validation size: 21\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class NucleiDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, data_dir: str, transforms: A.Compose = None):\n",
    "        self.df = df\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = transforms\n",
    "        self.target_classes = TARGET_CLASSES # Inherited from Section 2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_id = row['image_id']\n",
    "        H, W = row['Height'], row['Width']\n",
    "        \n",
    "        # 1. Load Image\n",
    "        image_path = os.path.join(self.data_dir, f\"{image_id}.tif\")\n",
    "        image = np.array(Image.open(image_path).convert(\"RGB\")) # Convert to RGB if needed\n",
    "\n",
    "        # 2. Decode RLE Masks for all classes\n",
    "        instance_masks: Dict[str, np.ndarray] = {}\n",
    "        for class_name in self.target_classes:\n",
    "            rle_string = row[class_name]\n",
    "            instance_masks[class_name] = rle_decode_instance_mask(rle_string, (H, W))\n",
    "\n",
    "        # 3. Generate Target Maps (Semantic, Distance, Class)\n",
    "        target_maps = generate_target_maps(instance_masks, (H, W))\n",
    "        \n",
    "        # Split target maps for augmentation\n",
    "        semantic_mask = target_maps[:, :, 0]\n",
    "        dist_map = target_maps[:, :, 1]\n",
    "        class_map = target_maps[:, :, 2]\n",
    "        \n",
    "        # Stack targets for augmentation: (H, W, 3)\n",
    "        # Note: Class map should be handled carefully if using color/stain augmentations.\n",
    "        # Here we stack [Image, Semantic Mask, Distance Map, Class Map]\n",
    "        augmented = None\n",
    "        if self.transforms:\n",
    "            # Augmentation applies simultaneously to image and masks\n",
    "            augmented = self.transforms(image=image, masks=[semantic_mask, dist_map, class_map])\n",
    "            image = augmented['image']\n",
    "            \n",
    "            # The masks list is returned in the order they were passed\n",
    "            semantic_mask = augmented['masks'][0]\n",
    "            dist_map = augmented['masks'][1]\n",
    "            class_map = augmented['masks'][2]\n",
    "\n",
    "        # Convert to PyTorch tensors and appropriate shapes/types\n",
    "        image = image.float() / 255.0 # Normalize image\n",
    "        \n",
    "        # Targets are 2D, squeeze the HxW to (1, H, W) or keep as (H, W) for direct loss calculation\n",
    "        # Semantic/Dist maps for segmentation head\n",
    "        seg_target = torch.stack([torch.tensor(semantic_mask).float(), torch.tensor(dist_map).float()], dim=0) # (2, H, W)\n",
    "        \n",
    "        # Class map for classification head (long type for CrossEntropy)\n",
    "        class_target = torch.tensor(class_map).long() # (H, W)\n",
    "        \n",
    "        return {\n",
    "            'image': image,             # (3, H, W)\n",
    "            'seg_target': seg_target,   # (2, H, W) -> [Semantic, Distance]\n",
    "            'class_target': class_target, # (H, W) -> [0, 1, 2, 3, 4]\n",
    "            'image_id': image_id\n",
    "        }\n",
    "\n",
    "# --- Augmentation Pipeline ---\n",
    "train_transforms = A.Compose([\n",
    "    # Geometric Augmentations (Essential for Multi-Organ Robustness)\n",
    "    A.Resize(256, 256), # Resize to a fixed size for mini-batching\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=10, p=0.5),\n",
    "    \n",
    "    # Photometric Augmentations (Essential for H&E Stain Variation)\n",
    "    A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05, p=0.5),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    \n",
    "    # ToTensorV2 converts numpy arrays to PyTorch tensors and moves the channel dim\n",
    "    ToTensorV2(transpose_mask=False), # False to keep HxW for mask, we stack later\n",
    "])\n",
    "\n",
    "# --- Create Dataset and DataLoader ---\n",
    "# Split data (simplified 90/10 split)\n",
    "train_split_df = train_df.sample(frac=0.9, random_state=42)\n",
    "val_split_df = train_df.drop(train_split_df.index)\n",
    "\n",
    "train_dataset = NucleiDataset(train_split_df, TRAIN_DIR, transforms=train_transforms)\n",
    "val_dataset = NucleiDataset(val_split_df, TRAIN_DIR, transforms=A.Compose([\n",
    "    A.Resize(256, 256), # Only resize for validation, no heavy augmentations\n",
    "    ToTensorV2(transpose_mask=False),\n",
    "]))\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0) \n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nDataLoader created. Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f11d67b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_seg_channels, out_class_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.inc = DoubleConv(in_channels, 64)\n",
    "        self.down1 = nn.MaxPool2d(2)\n",
    "        self.conv1 = DoubleConv(64, 128)\n",
    "        self.down2 = nn.MaxPool2d(2)\n",
    "        self.conv2 = DoubleConv(128, 256)\n",
    "        self.down3 = nn.MaxPool2d(2)\n",
    "        self.conv3 = DoubleConv(256, 512)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv_up2 = DoubleConv(512, 256)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv_up3 = DoubleConv(256, 128)\n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv_up4 = DoubleConv(128, 64)\n",
    "        \n",
    "        # Output Heads (from the final upsampled feature map)\n",
    "        self.seg_head = nn.Conv2d(64, out_seg_channels, kernel_size=1)\n",
    "        self.class_head = nn.Conv2d(64, out_class_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder passes\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.conv1(self.down1(x1))\n",
    "        x3 = self.conv2(self.down2(x2))\n",
    "        x4 = self.conv3(self.down3(x3))\n",
    "        \n",
    "        # Decoder passes (with skip connections)\n",
    "        x = self.up2(x4)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "        \n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "        \n",
    "        x = self.up4(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.conv_up4(x)\n",
    "        \n",
    "        # Output Heads\n",
    "        seg_output = self.seg_head(x)\n",
    "        class_output = self.class_head(x)\n",
    "        \n",
    "        return seg_output, class_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d83892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.21.0)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.9.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading torchaudio-2.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.2 kB)\n",
      "  Downloading torchaudio-2.7.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "  Downloading torchaudio-2.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "  Downloading torchaudio-2.6.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Downloading torchaudio-2.6.0-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchaudio\n",
      "Successfully installed torchaudio-2.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99cd7b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon (MPS) backend for GPU acceleration.\n",
      "Class weights initialized and moved to mps.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ----------------- CORRECTED DEVICE SETUP -----------------\n",
    "# 1. Check for Apple Silicon (MPS) support\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon (MPS) backend for GPU acceleration.\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"MPS backend not available. Falling back to CPU.\")\n",
    "\n",
    "# ----------------- CORRECTED CLASS_WEIGHTS INITIALIZATION -----------------\n",
    "\n",
    "# 5 classes: Background (0), Epithelial (1), Lymphocyte (2), Macrophage (3), Neutrophil (4)\n",
    "# Class Order: [Bkg, Epithelial, Lymphocyte, Macrophage, Neutrophil]\n",
    "\n",
    "# Using estimated, manually set weights reflecting the problem structure:\n",
    "CLASS_WEIGHTS = torch.tensor([0.1, 1.0, 1.0, 10.0, 10.0]).to(DEVICE) # <-- Use .to(DEVICE)\n",
    "print(f\"Class weights initialized and moved to {DEVICE}.\")\n",
    "\n",
    "# ----------------- CRITERION FUNCTION (No change needed inside) -----------------\n",
    "\n",
    "def criterion(seg_output, class_output, seg_target, class_target):\n",
    "    # ... (function body remains the same)\n",
    "    # The function uses F.cross_entropy which handles the weights on the device correctly.\n",
    "    # ...\n",
    "    # Placeholder for the rest of the function (assuming it's defined in your notebook)\n",
    "    pass\n",
    "# Define the actual criterion function here if you are re-running this cell:\n",
    "def criterion(seg_output, class_output, seg_target, class_target):\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    # 1. Segmentation Loss (MSE for Semantic + Distance Map)\n",
    "    seg_loss = F.mse_loss(seg_output, seg_target)\n",
    "\n",
    "    # 2. Classification Loss (Weighted Cross Entropy)\n",
    "    class_output_flat = class_output.permute(0, 2, 3, 1).reshape(-1, class_output.size(1))\n",
    "    class_target_flat = class_target.reshape(-1)\n",
    "    \n",
    "    class_loss = F.cross_entropy(\n",
    "        class_output_flat, \n",
    "        class_target_flat, \n",
    "        weight=CLASS_WEIGHTS, # CLASS_WEIGHTS is already on DEVICE\n",
    "        ignore_index=0\n",
    "    )\n",
    "    \n",
    "    # Combined Loss\n",
    "    total_loss = seg_loss + 0.5 * class_loss\n",
    "    \n",
    "    return total_loss, seg_loss, class_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d7bed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "def post_process_watershed(seg_output_np, class_output_np, min_size: int = 10) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Applies Watershed segmentation to separate instances and uses the class prediction.\n",
    "    \n",
    "    Args:\n",
    "        seg_output_np: The (2, H, W) segmentation output (Semantic, Distance)\n",
    "        class_output_np: The (5, H, W) class prediction output (Logits)\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary of {Class_Name: Instance_Mask}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Semantic Mask and Distance Map from U-Net output\n",
    "    semantic_pred = (seg_output_np[0] > 0.5).astype(np.uint8) # Nuclei mask from semantic head\n",
    "    dist_map_pred = seg_output_np[1] # Distance map from distance head\n",
    "\n",
    "    # 2. Find Markers (Seeds) for Watershed\n",
    "    # Markers are often the local maxima of the distance map, constrained by the semantic mask\n",
    "    local_maxima = (dist_map_pred == distance_transform_edt(dist_map_pred, sampling=[1, 1])) * semantic_pred\n",
    "    markers, num_markers = label(local_maxima)\n",
    "    \n",
    "    # If no markers found, return empty masks\n",
    "    if num_markers == 0:\n",
    "        return {c: np.zeros(semantic_pred.shape, dtype=np.uint16) for c in TARGET_CLASSES}\n",
    "    \n",
    "    # 3. Apply Watershed\n",
    "    # The distance map is inverted to act as a \"landscape\" where seeds flow to basins\n",
    "    # We use the negative distance map for the segmentation function\n",
    "    watershed_labels = watershed(-dist_map_pred, markers, mask=semantic_pred)\n",
    "\n",
    "    # 4. Classification\n",
    "    final_masks: Dict[str, np.ndarray] = {c: np.zeros(semantic_pred.shape, dtype=np.uint16) for c in TARGET_CLASSES}\n",
    "    class_id_to_name = {1: 'Epithelial', 2: 'Lymphocyte', 3: 'Macrophage', 4: 'Neutrophil'}\n",
    "\n",
    "    # Get the class prediction for each pixel (Bkg: 0, Epithelial: 1, ...)\n",
    "    # The classification head output (C, H, W) are logits. We take the argmax.\n",
    "    class_pred_map = np.argmax(class_output_np, axis=0)\n",
    "\n",
    "    # Iterate through all detected instances\n",
    "    for region in regionprops(watershed_labels):\n",
    "        inst_id = region.label\n",
    "        coords = region.coords\n",
    "\n",
    "        # Ensure the instance is not too small (filter noise)\n",
    "        if len(coords) < min_size:\n",
    "            continue\n",
    "            \n",
    "        # Determine the class for the entire instance\n",
    "        # Use the majority vote within the instance mask based on the class prediction map\n",
    "        instance_classes = class_pred_map[coords[:, 0], coords[:, 1]]\n",
    "        \n",
    "        # Filter out background (class 0) from the votes\n",
    "        instance_classes_nuclei = instance_classes[instance_classes > 0]\n",
    "        if len(instance_classes_nuclei) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Majority vote for the instance's final class\n",
    "        final_class_id = np.argmax(np.bincount(instance_classes_nuclei))\n",
    "        \n",
    "        if final_class_id > 0:\n",
    "            class_name = class_id_to_name[final_class_id]\n",
    "            \n",
    "            # Assign the instance to the correct class mask (using the original watershed ID)\n",
    "            # IMPORTANT: The instance ID must be unique *per class mask*\n",
    "            # We track the max ID for the current class mask\n",
    "            current_max_id = final_masks[class_name].max()\n",
    "            new_inst_id = current_max_id + 1\n",
    "            \n",
    "            # Apply the new ID to the final class mask\n",
    "            for r, c in coords:\n",
    "                final_masks[class_name][r, c] = new_inst_id\n",
    "                \n",
    "    return final_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16936b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon (MPS) backend for GPU acceleration.\n",
      "\n",
      "Starting training on mps for 10 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3y/x88zbv4n0b18f8cg1qcv1my40000gn/T/ipykernel_34309/1734601843.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  seg_target = torch.stack([torch.tensor(semantic_mask).float(), torch.tensor(dist_map).float()], dim=0) # (2, H, W)\n",
      "/var/folders/3y/x88zbv4n0b18f8cg1qcv1my40000gn/T/ipykernel_34309/1734601843.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_target = torch.tensor(class_map).long() # (H, W)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.9179 (Seg: 0.1048, Class: 1.4659)\n",
      "Epoch 2/10, Loss: 0.7261 (Seg: 0.1189, Class: 1.4899)\n",
      "Epoch 3/10, Loss: 0.6871 (Seg: 0.0597, Class: 1.2909)\n",
      "Epoch 4/10, Loss: 0.6332 (Seg: 0.0829, Class: 1.3233)\n",
      "Epoch 5/10, Loss: 0.5967 (Seg: 0.0860, Class: 0.9329)\n",
      "Epoch 6/10, Loss: 0.5325 (Seg: 0.0600, Class: 1.6381)\n",
      "Epoch 7/10, Loss: 0.5448 (Seg: 0.0437, Class: 0.9104)\n",
      "Epoch 8/10, Loss: 0.5195 (Seg: 0.0385, Class: 0.6598)\n",
      "Epoch 9/10, Loss: 0.4842 (Seg: 0.0544, Class: 0.6249)\n",
      "Epoch 10/10, Loss: 0.4862 (Seg: 0.0679, Class: 0.4931)\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 1. Check for Apple Silicon (MPS) support\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon (MPS) backend for GPU acceleration.\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"MPS backend not available. Falling back to CPU.\")\n",
    "\n",
    "# 2. Initialize and move model to the correct device\n",
    "model = SimpleUNet(\n",
    "    in_channels=3, \n",
    "    out_seg_channels=2, # Semantic, Distance\n",
    "    out_class_channels=5 # Bkg, Epithelial, Lymphocyte, Macrophage, Neutrophil\n",
    ").to(DEVICE) # <-- Uses the dynamically determined DEVICE ('mps' or 'cpu')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "NUM_EPOCHS = 10 \n",
    "# In a real competition, this would be much higher, with early stopping.\n",
    "\n",
    "print(f\"\\nStarting training on {DEVICE} for {NUM_EPOCHS} epochs...\")\n",
    "\n",
    "# --- Simplified Training Loop ---\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        images = data['image'].to(DEVICE)\n",
    "        seg_targets = data['seg_target'].to(DEVICE)\n",
    "        class_targets = data['class_target'].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        seg_output, class_output = model(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss, seg_loss, class_loss = criterion(seg_output, class_output, seg_targets, class_targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f} (Seg: {seg_loss.item():.4f}, Class: {class_loss.item():.4f})\")\n",
    "    \n",
    "    # (Optional: Add validation step with wPQ calculation here)\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07507dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining image dimensions...\n",
      "Warning: Image file not found for slide41\n",
      "Warning: Image file not found for slide42\n",
      "Warning: Image file not found for slide43\n",
      "Warning: Image file not found for slide44\n",
      "Warning: Image file not found for slide45\n",
      "Warning: Image file not found for slide46\n",
      "Warning: Image file not found for slide47\n",
      "Warning: Image file not found for slide48\n",
      "Warning: Image file not found for slide49\n",
      "Warning: Image file not found for slide50\n",
      "Warning: Image file not found for slide51\n",
      "Warning: Image file not found for slide52\n",
      "Warning: Image file not found for slide53\n",
      "Warning: Image file not found for slide54\n",
      "Warning: Image file not found for slide55\n",
      "Warning: Image file not found for slide56\n",
      "Warning: Image file not found for slide57\n",
      "Warning: Image file not found for slide58\n",
      "Warning: Image file not found for slide59\n",
      "Warning: Image file not found for slide60\n",
      "Warning: Image file not found for slide61\n",
      "Warning: Image file not found for slide62\n",
      "Warning: Image file not found for slide63\n",
      "Warning: Image file not found for slide64\n",
      "Warning: Image file not found for slide65\n",
      "Warning: Image file not found for slide66\n",
      "Warning: Image file not found for slide67\n",
      "Warning: Image file not found for slide68\n",
      "Warning: Image file not found for slide69\n",
      "Warning: Image file not found for slide70\n",
      "Warning: Image file not found for slide71\n",
      "Warning: Image file not found for slide72\n",
      "Warning: Image file not found for slide73\n",
      "Warning: Image file not found for slide74\n",
      "Warning: Image file not found for slide75\n",
      "Warning: Image file not found for slide76\n",
      "Warning: Image file not found for slide77\n",
      "Warning: Image file not found for slide78\n",
      "Warning: Image file not found for slide79\n",
      "Warning: Image file not found for slide80\n",
      "Warning: Image file not found for slide81\n",
      "Warning: Image file not found for slide82\n",
      "Warning: Image file not found for slide83\n",
      "Warning: Image file not found for slide84\n",
      "Warning: Image file not found for slide85\n",
      "Warning: Image file not found for slide86\n",
      "Warning: Image file not found for slide87\n",
      "Warning: Image file not found for slide88\n",
      "Warning: Image file not found for slide89\n",
      "Warning: Image file not found for slide90\n",
      "Warning: Image file not found for slide91\n",
      "Warning: Image file not found for slide92\n",
      "Warning: Image file not found for slide93\n",
      "Warning: Image file not found for slide94\n",
      "Warning: Image file not found for slide95\n",
      "Warning: Image file not found for slide96\n",
      "Warning: Image file not found for slide97\n",
      "Warning: Image file not found for slide98\n",
      "Warning: Image file not found for slide99\n",
      "Warning: Image file not found for slide100\n",
      "Warning: Image file not found for slide101\n",
      "Warning: Image file not found for slide102\n",
      "Warning: Image file not found for slide103\n",
      "Warning: Image file not found for slide104\n",
      "Warning: Image file not found for slide105\n",
      "Warning: Image file not found for slide106\n",
      "Warning: Image file not found for slide107\n",
      "Warning: Image file not found for slide108\n",
      "Warning: Image file not found for slide109\n",
      "Warning: Image file not found for slide110\n",
      "Warning: Image file not found for slide111\n",
      "Warning: Image file not found for slide112\n",
      "Warning: Image file not found for slide113\n",
      "Warning: Image file not found for slide114\n",
      "Warning: Image file not found for slide115\n",
      "Warning: Image file not found for slide116\n",
      "Warning: Image file not found for slide117\n",
      "Warning: Image file not found for slide118\n",
      "Warning: Image file not found for slide119\n",
      "Warning: Image file not found for slide120\n",
      "Warning: Image file not found for slide121\n",
      "Warning: Image file not found for slide122\n",
      "Warning: Image file not found for slide123\n",
      "Warning: Image file not found for slide124\n",
      "Warning: Image file not found for slide125\n",
      "Warning: Image file not found for slide126\n",
      "Warning: Image file not found for slide127\n",
      "Warning: Image file not found for slide128\n",
      "Warning: Image file not found for slide129\n",
      "Warning: Image file not found for slide130\n",
      "Warning: Image file not found for slide131\n",
      "Warning: Image file not found for slide132\n",
      "Warning: Image file not found for slide133\n",
      "Warning: Image file not found for slide134\n",
      "Warning: Image file not found for slide135\n",
      "Warning: Image file not found for slide136\n",
      "Warning: Image file not found for slide137\n",
      "Warning: Image file not found for slide138\n",
      "Warning: Image file not found for slide139\n",
      "Warning: Image file not found for slide140\n",
      "Warning: Image file not found for slide141\n",
      "Warning: Image file not found for slide142\n",
      "Warning: Image file not found for slide143\n",
      "Warning: Image file not found for slide144\n",
      "Warning: Image file not found for slide145\n",
      "Warning: Image file not found for slide146\n",
      "Warning: Image file not found for slide147\n",
      "Warning: Image file not found for slide148\n",
      "Warning: Image file not found for slide149\n",
      "Warning: Image file not found for slide150\n",
      "Warning: Image file not found for slide151\n",
      "Warning: Image file not found for slide152\n",
      "Warning: Image file not found for slide153\n",
      "Warning: Image file not found for slide154\n",
      "Warning: Image file not found for slide155\n",
      "Warning: Image file not found for slide156\n",
      "Warning: Image file not found for slide157\n",
      "Warning: Image file not found for slide158\n",
      "Warning: Image file not found for slide159\n",
      "Warning: Image file not found for slide160\n",
      "Warning: Image file not found for slide161\n",
      "Warning: Image file not found for slide162\n",
      "Warning: Image file not found for slide163\n",
      "Warning: Image file not found for slide164\n",
      "Warning: Image file not found for slide165\n",
      "Warning: Image file not found for slide166\n",
      "Warning: Image file not found for slide167\n",
      "Warning: Image file not found for slide168\n",
      "Warning: Image file not found for slide169\n",
      "Warning: Image file not found for slide170\n",
      "Warning: Image file not found for slide171\n",
      "Warning: Image file not found for slide172\n",
      "Warning: Image file not found for slide173\n",
      "Warning: Image file not found for slide174\n",
      "Warning: Image file not found for slide175\n",
      "Warning: Image file not found for slide176\n",
      "Warning: Image file not found for slide177\n",
      "Warning: Image file not found for slide178\n",
      "Warning: Image file not found for slide179\n",
      "Warning: Image file not found for slide180\n",
      "Warning: Image file not found for slide181\n",
      "Warning: Image file not found for slide182\n",
      "Warning: Image file not found for slide183\n",
      "Warning: Image file not found for slide184\n",
      "Warning: Image file not found for slide185\n",
      "Warning: Image file not found for slide186\n",
      "Warning: Image file not found for slide187\n",
      "Warning: Image file not found for slide188\n",
      "Warning: Image file not found for slide189\n",
      "Warning: Image file not found for slide190\n",
      "Warning: Image file not found for slide191\n",
      "Warning: Image file not found for slide192\n",
      "Warning: Image file not found for slide193\n",
      "Warning: Image file not found for slide194\n",
      "Warning: Image file not found for slide195\n",
      "Warning: Image file not found for slide196\n",
      "Warning: Image file not found for slide197\n",
      "Warning: Image file not found for slide198\n",
      "Warning: Image file not found for slide199\n",
      "Warning: Image file not found for slide200\n",
      "Warning: Image file not found for slide201\n",
      "Warning: Image file not found for slide202\n",
      "Warning: Image file not found for slide203\n",
      "Warning: Image file not found for slide204\n",
      "Warning: Image file not found for slide205\n",
      "Warning: Image file not found for slide206\n",
      "Warning: Image file not found for slide207\n",
      "Warning: Image file not found for slide208\n",
      "Warning: Image file not found for slide209\n",
      "Loaded annotations for 209 images.\n",
      "\n",
      "Loaded 40 test images.\n",
      "  image_id                                         Epithelial  \\\n",
      "0   slide1                                                  0   \n",
      "1   slide2  191 3596 1 191 4378 14 191 5162 15 191 5947 16...   \n",
      "2   slide3                                                  0   \n",
      "3   slide4  17 606 1 17 1114 9 61 1425 21 17 1624 13 61 19...   \n",
      "4   slide5  1 106286 7 1 106708 9 1 107131 11 1 107553 13 ...   \n",
      "\n",
      "                                          Lymphocyte  \\\n",
      "0                                                  0   \n",
      "1  6 15974 1 1 16131 2 6 16755 8 1 16911 8 6 1753...   \n",
      "2                                                  0   \n",
      "3                                                  0   \n",
      "4  70 1748 2 69 2128 3 70 2168 8 69 2550 7 70 259...   \n",
      "\n",
      "                                          Neutrophil  \\\n",
      "0                                                  0   \n",
      "1                                                  0   \n",
      "2  2 4323 8 2 4552 12 2 4781 15 2 5010 18 2 5240 ...   \n",
      "3                                                  0   \n",
      "4                                                  0   \n",
      "\n",
      "                                          Macrophage  Height  Width  \n",
      "0  5 2479 3 5 2775 7 5 3070 14 5 3366 22 5 3663 2...     157    219  \n",
      "1                                                  0     950    310  \n",
      "2                                                  0     567    796  \n",
      "3                                                  0     134    124  \n",
      "4                                                  0    2150   1401  \n",
      "Loading test image: kaggle-data/test_final/slide1.tif\n",
      "Loading test image: kaggle-data/test_final/slide2.tif\n",
      "Loading test image: kaggle-data/test_final/slide3.tif\n",
      "Loading test image: kaggle-data/test_final/slide4.tif\n",
      "Loading test image: kaggle-data/test_final/slide5.tif\n",
      "Loading test image: kaggle-data/test_final/slide6.tif\n",
      "Loading test image: kaggle-data/test_final/slide7.tif\n",
      "Loading test image: kaggle-data/test_final/slide8.tif\n",
      "Loading test image: kaggle-data/test_final/slide9.tif\n",
      "Loading test image: kaggle-data/test_final/slide10.tif\n",
      "Loading test image: kaggle-data/test_final/slide11.tif\n",
      "Loading test image: kaggle-data/test_final/slide12.tif\n",
      "Loading test image: kaggle-data/test_final/slide13.tif\n",
      "Loading test image: kaggle-data/test_final/slide14.tif\n",
      "Loading test image: kaggle-data/test_final/slide15.tif\n",
      "Loading test image: kaggle-data/test_final/slide16.tif\n",
      "Loading test image: kaggle-data/test_final/slide17.tif\n",
      "Loading test image: kaggle-data/test_final/slide18.tif\n",
      "Loading test image: kaggle-data/test_final/slide19.tif\n",
      "Loading test image: kaggle-data/test_final/slide20.tif\n",
      "Loading test image: kaggle-data/test_final/slide21.tif\n",
      "Loading test image: kaggle-data/test_final/slide22.tif\n",
      "Loading test image: kaggle-data/test_final/slide23.tif\n",
      "Loading test image: kaggle-data/test_final/slide24.tif\n",
      "Loading test image: kaggle-data/test_final/slide25.tif\n",
      "Loading test image: kaggle-data/test_final/slide26.tif\n",
      "Loading test image: kaggle-data/test_final/slide27.tif\n",
      "Loading test image: kaggle-data/test_final/slide28.tif\n",
      "Loading test image: kaggle-data/test_final/slide29.tif\n",
      "Loading test image: kaggle-data/test_final/slide30.tif\n",
      "Loading test image: kaggle-data/test_final/slide31.tif\n",
      "Loading test image: kaggle-data/test_final/slide32.tif\n",
      "Loading test image: kaggle-data/test_final/slide33.tif\n",
      "Loading test image: kaggle-data/test_final/slide34.tif\n",
      "Loading test image: kaggle-data/test_final/slide35.tif\n",
      "Loading test image: kaggle-data/test_final/slide36.tif\n",
      "Loading test image: kaggle-data/test_final/slide37.tif\n",
      "Loading test image: kaggle-data/test_final/slide38.tif\n",
      "Loading test image: kaggle-data/test_final/slide39.tif\n",
      "Loading test image: kaggle-data/test_final/slide40.tif\n",
      "\n",
      "Submission file created at: submission.csv\n",
      "  image_id Epithelial Lymphocyte Neutrophil Macrophage\n",
      "0   slide1          0          0          0          0\n",
      "1   slide2          0          0          0          0\n",
      "2   slide3          0          0          0          0\n",
      "3   slide4          0          0          0          0\n",
      "4   slide5          0          0          0          0\n"
     ]
    }
   ],
   "source": [
    "# Load Test Data\n",
    "TEST_DF = pd.DataFrame({'image_id': [os.path.splitext(f)[0] for f in os.listdir(TEST_DIR) if f.endswith('.tif')]})\n",
    "\n",
    "# Determine test image shapes\n",
    "TEST_DF = load_and_decode_masks(os.path.join(DATA_DIR, \"train_ground_truth.csv\"), TEST_DIR)\n",
    "# Note: Re-use the load_and_decode_masks function as it includes the shape discovery logic. \n",
    "# We need to manually filter the DF to just test images if the directory listing is used.\n",
    "\n",
    "# For simplicity, assume all images in TEST_DIR were added to TEST_DF with correct shapes.\n",
    "TEST_DF = TEST_DF[TEST_DF['image_id'].isin([os.path.splitext(f)[0] for f in os.listdir(TEST_DIR) if f.endswith('.tif')])]\n",
    "\n",
    "print(f\"\\nLoaded {len(TEST_DF)} test images.\")\n",
    "print(TEST_DF.head())\n",
    "\n",
    "class TestNucleiDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, data_dir: str):\n",
    "        self.df = df\n",
    "        self.data_dir = data_dir\n",
    "        self.transforms = A.Compose([A.Resize(256, 256), ToTensorV2()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_id = row['image_id']\n",
    "        image_path = os.path.join(self.data_dir, f\"{image_id}.tif\")\n",
    "        # print(f\"Loading test image: {image_path}\")\n",
    "        image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "        \n",
    "        # The image needs to be stored *before* the transform for correct RLE encoding later\n",
    "        original_shape = image.shape[:2]\n",
    "        \n",
    "        augmented = self.transforms(image=image)\n",
    "        image_tensor = augmented['image'].float() / 255.0\n",
    "        \n",
    "        return {\n",
    "            'image': image_tensor,\n",
    "            'image_id': image_id,\n",
    "            'original_shape': original_shape\n",
    "        }\n",
    "\n",
    "test_dataset = TestNucleiDataset(TEST_DF, TEST_DIR)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- Submission Generation ---\n",
    "submission_data = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        image = data['image'].to(DEVICE)\n",
    "        image_id = data['image_id'][0]\n",
    "        # Original shape (H, W) for RLE encoding\n",
    "        orig_H, orig_W = data['original_shape'][0].item(), data['original_shape'][1].item() \n",
    "\n",
    "        # Forward pass\n",
    "        seg_output, class_output = model(image)\n",
    "\n",
    "        # Convert outputs to numpy and resize back to original shape (critical!)\n",
    "        seg_output_np = seg_output.cpu().squeeze(0).numpy()\n",
    "        class_output_np = class_output.cpu().squeeze(0).numpy()\n",
    "        \n",
    "        # Resize output back to original image size\n",
    "        # Use bilinear interpolation for continuous maps (seg) and nearest for class logits\n",
    "        from skimage.transform import resize\n",
    "        \n",
    "        resized_seg_output = np.stack([\n",
    "            resize(seg_output_np[i], (orig_H, orig_W), order=1, anti_aliasing=False) \n",
    "            for i in range(seg_output_np.shape[0])\n",
    "        ], axis=0)\n",
    "\n",
    "        resized_class_output = np.stack([\n",
    "            resize(class_output_np[i], (orig_H, orig_W), order=0, anti_aliasing=False) \n",
    "            for i in range(class_output_np.shape[0])\n",
    "        ], axis=0)\n",
    "        \n",
    "        # Post-process (Watershed) to get instance masks\n",
    "        final_instance_masks = post_process_watershed(resized_seg_output, resized_class_output)\n",
    "        \n",
    "        # RLE Encode the final masks\n",
    "        rle_row = {'image_id': image_id}\n",
    "        for class_name in TARGET_CLASSES:\n",
    "            mask = final_instance_masks[class_name]\n",
    "            rle_row[class_name] = rle_encode_instance_mask(mask)\n",
    "            \n",
    "        submission_data.append(rle_row)\n",
    "\n",
    "# Create final submission CSV\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "# Ensure columns are in the correct order: image_id,Epithelial,Lymphocyte,Neutrophil,Macrophage\n",
    "SUBMISSION_COLUMNS = ['image_id', 'Epithelial', 'Lymphocyte', 'Neutrophil', 'Macrophage']\n",
    "submission_df = submission_df[SUBMISSION_COLUMNS]\n",
    "\n",
    "# Save the submission file\n",
    "SUBMISSION_FILE_PATH = 'submission.csv'\n",
    "submission_df.to_csv(SUBMISSION_FILE_PATH, index=False)\n",
    "print(f\"\\nSubmission file created at: {SUBMISSION_FILE_PATH}\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
