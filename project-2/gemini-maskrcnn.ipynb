{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845300d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.9.0\n",
      "Torchvision Version: 0.24.0\n",
      "Using device: mps\n",
      "Starting training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 271\u001b[39m\n\u001b[32m    268\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSubmission file \u001b[39m\u001b[33m'\u001b[39m\u001b[33msubmission_v2.csv\u001b[39m\u001b[33m'\u001b[39m\u001b[33m created successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 211\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# **IMPROVEMENT**: Updated training loop with validation\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config.NUM_EPOCHS):\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m     val_loss = evaluate(model, data_loader_val, device)\n\u001b[32m    214\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 154\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, optimizer, data_loader, device, epoch)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_one_epoch\u001b[39m(model, optimizer, data_loader, device, epoch):\n\u001b[32m    153\u001b[39m     model.train()\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mcontinue\u001b[39;49;00m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/User/Aditya/NYU/Semester/Fall-2025/Courses/Computer Vision/Projects/Repository/computer-vision/cell_project_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/User/Aditya/NYU/Semester/Fall-2025/Courses/Computer Vision/Projects/Repository/computer-vision/cell_project_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/User/Aditya/NYU/Semester/Fall-2025/Courses/Computer Vision/Projects/Repository/computer-vision/cell_project_env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mNucleiDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     72\u001b[39m img_path = os.path.join(\u001b[38;5;28mself\u001b[39m.image_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.tif\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     73\u001b[39m xml_path = os.path.join(\u001b[38;5;28mself\u001b[39m.image_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.xml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m tree = ET.parse(xml_path)\n\u001b[32m     77\u001b[39m root = tree.getroot()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/User/Aditya/NYU/Semester/Fall-2025/Courses/Computer Vision/Projects/Repository/computer-vision/cell_project_env/lib/python3.12/site-packages/PIL/Image.py:972\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert\u001b[39m(\n\u001b[32m    921\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    922\u001b[39m     mode: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    926\u001b[39m     colors: \u001b[38;5;28mint\u001b[39m = \u001b[32m256\u001b[39m,\n\u001b[32m    927\u001b[39m ) -> Image:\n\u001b[32m    928\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[33;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[32m    930\u001b[39m \u001b[33;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    969\u001b[39m \u001b[33;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m     has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    975\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    976\u001b[39m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/User/Aditya/NYU/Semester/Fall-2025/Courses/Computer Vision/Projects/Repository/computer-vision/cell_project_env/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:1302\u001b[39m, in \u001b[36mTiffImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Image.core.PixelAccess | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tile \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_load_libtiff:\n\u001b[32m-> \u001b[39m\u001b[32m1302\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_libtiff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().load()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/User/Aditya/NYU/Semester/Fall-2025/Courses/Computer Vision/Projects/Repository/computer-vision/cell_project_env/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:1401\u001b[39m, in \u001b[36mTiffImageFile._load_libtiff\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1399\u001b[39m     pos = os.lseek(fp, \u001b[32m0\u001b[39m, os.SEEK_CUR)\n\u001b[32m   1400\u001b[39m     \u001b[38;5;66;03m# 4 bytes, otherwise the trace might error out\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1401\u001b[39m     n, err = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfpfp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1402\u001b[39m     os.lseek(fp, pos, os.SEEK_SET)\n\u001b[32m   1403\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1404\u001b[39m     \u001b[38;5;66;03m# we have something else.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# run_nuclei_segmentation.py\n",
    "\n",
    "import os\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision import transforms as T\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Torchvision Version: {torchvision.__version__}\")\n",
    "\n",
    "# Set device to MPS (for Apple Silicon) or CUDA or CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "class Config:\n",
    "    \"\"\"Configuration settings for the project.\"\"\"\n",
    "    DATA_PATH = \"kaggle-data/\"\n",
    "    TRAIN_DIR = os.path.join(DATA_PATH, \"train\")\n",
    "    VAL_DIR = os.path.join(DATA_PATH, \"val\")\n",
    "    TEST_DIR = os.path.join(DATA_PATH, \"test_final\")\n",
    "\n",
    "    # Model parameters\n",
    "    NUM_CLASSES = 5  # 4 classes + 1 background\n",
    "    BATCH_SIZE = 2 # Lower if you run out of memory\n",
    "    NUM_EPOCHS = 15 # Increase for better results\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # Class mapping\n",
    "    CLASS_MAP = {\n",
    "        \"Epithelial\": 1,\n",
    "        \"Lymphocyte\": 2,\n",
    "        \"Macrophage\": 3,\n",
    "        \"Neutrophil\": 4,\n",
    "    }\n",
    "    # Inverse mapping for prediction visualization\n",
    "    INV_CLASS_MAP = {v: k for k, v in CLASS_MAP.items()}\n",
    "\n",
    "# Instantiate config\n",
    "config = Config()\n",
    "\n",
    "# --- 2. RLE Encoding Helper ---\n",
    "def rle_encode_instance_mask(mask: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Converts a 2D instance mask (where each object has a unique integer ID)\n",
    "    into a Run-Length Encoded (RLE) string for the submission format.\n",
    "    \"\"\"\n",
    "    pixels = mask.flatten(order=\"F\").astype(np.int32)\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "\n",
    "    rle = []\n",
    "    for i in range(0, len(runs) - 1):\n",
    "        start = runs[i]\n",
    "        length = runs[i+1] - start\n",
    "        val = pixels[start]\n",
    "        if val > 0:\n",
    "            rle.extend([val, start, length])\n",
    "\n",
    "    if not rle:\n",
    "        return \"0\"  # Return \"0\" if no instances are found\n",
    "\n",
    "    return \" \".join(map(str, rle))\n",
    "\n",
    "# --- 3. Custom Dataset ---\n",
    "class NucleiDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for loading nuclei images and their corresponding\n",
    "    XML annotations for instance segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_ids = sorted([f.split('.')[0] for f in os.listdir(image_dir) if f.endswith('.tif')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.image_dir, f\"{image_id}.tif\")\n",
    "        xml_path = os.path.join(self.image_dir, f\"{image_id}.xml\")\n",
    "\n",
    "        # Open image and convert to RGB\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Parse XML annotations\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        masks = []\n",
    "        labels = []\n",
    "        boxes = []\n",
    "\n",
    "        for annotation in root.findall(\".//Annotation\"):\n",
    "            label_name = annotation.get(\"Name\")\n",
    "            if label_name in config.CLASS_MAP:\n",
    "                label_id = config.CLASS_MAP[label_name]\n",
    "                for region in annotation.findall(\".//Region\"):\n",
    "                    vertices = []\n",
    "                    for vertex in region.findall(\".//Vertex\"):\n",
    "                        vertices.append((float(vertex.get(\"X\")), float(vertex.get(\"Y\"))))\n",
    "                    \n",
    "                    if not vertices:\n",
    "                        continue\n",
    "\n",
    "                    mask = np.zeros((img.height, img.width), dtype=np.uint8)\n",
    "                    pts = np.array(vertices, dtype=np.int32)\n",
    "                    cv2.fillPoly(mask, [pts], 1)\n",
    "                    \n",
    "                    pos = np.where(mask)\n",
    "                    if pos[0].size == 0 or pos[1].size == 0:\n",
    "                        continue\n",
    "\n",
    "                    xmin = np.min(pos[1])\n",
    "                    xmax = np.max(pos[1])\n",
    "                    ymin = np.min(pos[0])\n",
    "                    ymax = np.max(pos[0])\n",
    "                    \n",
    "                    if xmin >= xmax or ymin >= ymax:\n",
    "                        continue\n",
    "\n",
    "                    masks.append(mask)\n",
    "                    labels.append(label_id)\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # Create target dictionary\n",
    "        target = {}\n",
    "        if masks:\n",
    "            target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            target[\"masks\"] = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n",
    "            target[\"image_id\"] = torch.tensor([idx])\n",
    "            target[\"area\"] = (target[\"boxes\"][:, 3] - target[\"boxes\"][:, 1]) * (target[\"boxes\"][:, 2] - target[\"boxes\"][:, 0])\n",
    "            target[\"iscrowd\"] = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        else: # Handle images with no annotations\n",
    "            target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            target[\"labels\"] = torch.zeros(0, dtype=torch.int64)\n",
    "            target[\"masks\"] = torch.zeros((0, img.height, img.width), dtype=torch.uint8)\n",
    "            target[\"image_id\"] = torch.tensor([idx])\n",
    "            target[\"area\"] = torch.zeros(0, dtype=torch.float32)\n",
    "            target[\"iscrowd\"] = torch.zeros((0,), dtype=torch.int64)\n",
    "\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "# --- 4. Data Augmentation and Transforms ---\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        image = T.ToTensor()(image)\n",
    "        return image, target\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = [ToTensor()]\n",
    "    return Compose(transforms)\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Filters out images with no annotations and then prepares the batch.\n",
    "    \"\"\"\n",
    "    # Filter out samples where the target has no boxes.\n",
    "    batch = list(filter(lambda x: len(x[1][\"boxes\"]) > 0, batch))\n",
    "    # If the filtered batch is empty, return None.\n",
    "    if not batch:\n",
    "        return None, None\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# --- 5. Model Definition ---\n",
    "def get_model(num_classes):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained Mask R-CNN model and modifies its classification heads\n",
    "    for the specified number of classes.\n",
    "    \"\"\"\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    return model\n",
    "\n",
    "# --- 6. Training Logic ---\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    \"\"\"Main training loop for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    print_freq = 10\n",
    "    \n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "        # Skip iteration if the batch is empty after filtering\n",
    "        if images is None or not images:\n",
    "            continue\n",
    "            \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        if not torch.isfinite(losses):\n",
    "            print(f\"Skipping iteration {i} due to non-finite loss: {losses.item()}\")\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % print_freq == 0:\n",
    "            print(f\"Epoch [{epoch+1}], Iter [{i+1}/{len(data_loader)}], Loss: {losses.item():.4f}\")\n",
    "\n",
    "# --- 7. Main Execution Block ---\n",
    "def main():\n",
    "    dataset_train = NucleiDataset(config.TRAIN_DIR, get_transform(train=True))\n",
    "    dataset_val = NucleiDataset(config.VAL_DIR, get_transform(train=False))\n",
    "\n",
    "    data_loader_train = DataLoader(\n",
    "        dataset_train, batch_size=config.BATCH_SIZE, shuffle=True, \n",
    "        num_workers=0, collate_fn=collate_fn\n",
    "    )\n",
    "    data_loader_val = DataLoader(\n",
    "        dataset_val, batch_size=1, shuffle=False, \n",
    "        num_workers=0, collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    print(\"Datasets and DataLoaders created successfully.\")\n",
    "\n",
    "    model = get_model(config.NUM_CLASSES)\n",
    "    model.to(device)\n",
    "    print(\"Model loaded and moved to device.\")\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params, lr=config.LEARNING_RATE)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        train_one_epoch(model, optimizer, data_loader_train, device, epoch)\n",
    "        print(f\"--- Epoch {epoch+1} finished ---\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    \n",
    "    MODEL_SAVE_PATH = \"nuclei_segmentation_model.pth\"\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(f\"Model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "    print(\"Starting inference on the test set...\")\n",
    "    model.eval()\n",
    "    \n",
    "    test_image_ids = sorted([f.split('.')[0] for f in os.listdir(config.TEST_DIR) if f.endswith('.tif')])\n",
    "    submission_data = []\n",
    "\n",
    "    for image_id in test_image_ids:\n",
    "        img_path = os.path.join(config.TEST_DIR, f\"{image_id}.tif\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = T.ToTensor()(img).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model([img_tensor])\n",
    "\n",
    "        instance_maps = {cls_name: np.zeros((img.height, img.width), dtype=np.int32) \n",
    "                         for cls_name in config.CLASS_MAP.keys()}\n",
    "        \n",
    "        pred_scores = prediction[0]['scores'].cpu().numpy()\n",
    "        pred_labels = prediction[0]['labels'].cpu().numpy()\n",
    "        pred_masks = prediction[0]['masks'].cpu().numpy()\n",
    "\n",
    "        instance_counters = {cls_name: 1 for cls_name in config.CLASS_MAP.keys()}\n",
    "\n",
    "        score_threshold = 0.5\n",
    "        \n",
    "        for i in range(len(pred_scores)):\n",
    "            if pred_scores[i] > score_threshold:\n",
    "                label_id = pred_labels[i]\n",
    "                if label_id in config.INV_CLASS_MAP:\n",
    "                    class_name = config.INV_CLASS_MAP[label_id]\n",
    "                    mask = (pred_masks[i, 0] > 0.5).astype(np.uint8)\n",
    "                    instance_id = instance_counters[class_name]\n",
    "                    instance_maps[class_name][(mask == 1) & (instance_maps[class_name] == 0)] = instance_id\n",
    "                    instance_counters[class_name] += 1\n",
    "\n",
    "        rle_results = {}\n",
    "        for class_name, instance_map in instance_maps.items():\n",
    "            rle_results[class_name] = rle_encode_instance_mask(instance_map)\n",
    "            \n",
    "        submission_data.append({\n",
    "            'image_id': image_id,\n",
    "            'Epithelial': rle_results['Epithelial'],\n",
    "            'Lymphocyte': rle_results['Lymphocyte'],\n",
    "            'Macrophage': rle_results['Macrophage'],\n",
    "            'Neutrophil': rle_results['Neutrophil'],\n",
    "        })\n",
    "        print(f\"Processed test image: {image_id}\")\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    submission_df = submission_df[['image_id', 'Epithelial', 'Lymphocyte', 'Neutrophil', 'Macrophage']]\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "    print(\"\\nSubmission file 'submission.csv' created successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
