{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e5b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install stardist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb7194",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cellpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1473156",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cellseg_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "449b0ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS (GPU) ðŸš€\n",
      "--- Initializing Model and DataLoaders ---\n",
      "ðŸ”§ Updated 'nuc_type' head to 5 classes.\n",
      "--- Starting Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]:   0%|          | 0/53 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'nuc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 242\u001b[39m\n\u001b[32m    239\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… New best model saved with validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(CONFIG.MODEL_SAVE_PATH):\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     \u001b[43mtrain_and_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    244\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel already trained. Found at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG.MODEL_SAVE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 189\u001b[39m, in \u001b[36mtrain_and_validate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    186\u001b[39m type_maps = labels[:, \u001b[32m1\u001b[39m, :, :].to(DEVICE)  \u001b[38;5;66;03m# type map\u001b[39;00m\n\u001b[32m    188\u001b[39m outputs = model.model(images)  \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m pred_inst_maps = \u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnuc\u001b[49m.nuc_map      \u001b[38;5;66;03m# predicted instance maps\u001b[39;00m\n\u001b[32m    190\u001b[39m pred_type_maps = outputs.type.nuc_type     \u001b[38;5;66;03m# predicted type maps\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# --- Compute losses manually ---\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Use CrossEntropy for type maps\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'nuc'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Project 2: Nuclei Segmentation with HoVer-Net (End-to-End)\n",
    "#\n",
    "# This notebook implements a robust, end-to-end pipeline using the **HoVer-Net** model. This approach is more powerful and reliable than a two-stage process.\n",
    "#\n",
    "# 1.  **Data Loading**: We create a PyTorch Dataset that loads images and their corresponding XML annotations.\n",
    "# 2.  **Fine-Tuning**: A pre-trained HoVer-Net model is fine-tuned on the `train` set.\n",
    "# 3.  **Validation**: The `val` set is used after each epoch to check performance and save the best model.\n",
    "# 4.  **Inference**: The best model is used to generate predictions on the `test` set.\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations import Resize, Compose, Normalize\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import warnings\n",
    "\n",
    "# --- Import HoVer-Net Model ---\n",
    "from cellseg_models_pytorch.models import hovernet\n",
    "\n",
    "# --- Suppress unnecessary warnings ---\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Configuration and Device Setup\n",
    "#\n",
    "# Here, we define all paths and hyperparameters. The code will automatically detect and use your Apple Silicon (MPS) GPU if available.\n",
    "\n",
    "# %%\n",
    "class CONFIG:\n",
    "    # --- Paths (including the validation set) ---\n",
    "    BASE_DIR = \"kaggle-data/\"\n",
    "    TRAIN_IMG_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "    TRAIN_XML_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "    VAL_IMG_DIR = os.path.join(BASE_DIR, \"val\")\n",
    "    VAL_XML_DIR = os.path.join(BASE_DIR, \"val\")\n",
    "    TEST_IMG_DIR = os.path.join(BASE_DIR, \"test_final\")\n",
    "\n",
    "    MODEL_SAVE_PATH = \"hovernet_best_model.pth\"\n",
    "\n",
    "    # --- Model & Training Parameters ---\n",
    "    IMG_SIZE = 256   # HoVer-Net is often trained on smaller patches\n",
    "    BATCH_SIZE = 4   # Use a smaller batch size for this larger model\n",
    "    EPOCHS = 30\n",
    "    LR = 1e-4\n",
    "\n",
    "    # --- Class Mapping (Important: 0 is background) ---\n",
    "    CLASSES = [\"Epirthelial\", \"Lymphocyte\", \"Macrophage\", \"Neutrophil\"]\n",
    "    CLASS_MAP = {name: i + 1 for i, name in enumerate(CLASSES)} # 1-based indexing for classes\n",
    "    INV_CLASS_MAP = {i + 1: name for i, name in enumerate(CLASSES)}\n",
    "\n",
    "# --- Setup Device (MPS for Apple Silicon, CPU fallback) ---\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using Apple MPS (GPU) ðŸš€\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Data Loading and Helper Functions\n",
    "#\n",
    "# These utilities prepare the data in the specific format required for training the HoVer-Net model.\n",
    "\n",
    "# %%\n",
    "def rle_encode_instance_mask(mask: np.ndarray) -> str:\n",
    "    \"\"\"Encodes a 2D instance mask to a RLE triple string.\"\"\"\n",
    "    pixels = mask.flatten(order=\"F\").astype(np.int32)\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    rle = []\n",
    "    for i in range(len(runs) - 1):\n",
    "        val, start, length = pixels[runs[i]], runs[i], runs[i+1] - runs[i]\n",
    "        if val > 0:\n",
    "            rle.extend([val, start, length])\n",
    "    return \" \".join(map(str, rle)) if rle else \"0\"\n",
    "\n",
    "def create_training_maps(xml_path, shape):\n",
    "    \"\"\"Parses XML to create instance and type maps for training.\"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    inst_map = np.zeros(shape, dtype=np.int16)\n",
    "    type_map = np.zeros(shape, dtype=np.int16)\n",
    "    inst_id_counter = 1\n",
    "\n",
    "    for ann_elem in root.findall('Annotation'):\n",
    "        class_name = ann_elem.get('Name')\n",
    "        if class_name in CONFIG.CLASS_MAP:\n",
    "            class_id = CONFIG.CLASS_MAP[class_name]\n",
    "            for region_elem in ann_elem.findall('.//Region'):\n",
    "                vertices = [(float(v.get('X')), float(v.get('Y'))) for v in region_elem.findall('.//Vertex')]\n",
    "                polygon = np.array(vertices, dtype=np.int32)\n",
    "                cv2.fillPoly(inst_map, [polygon], inst_id_counter)\n",
    "                cv2.fillPoly(type_map, [polygon], class_id)\n",
    "                inst_id_counter += 1\n",
    "    return inst_map, type_map\n",
    "\n",
    "class NucleiDataset(Dataset):\n",
    "    \"\"\"Custom PyTorch Dataset for loading images and masks for HoVer-Net.\"\"\"\n",
    "    def __init__(self, image_dir, xml_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.xml_dir = xml_dir\n",
    "        self.file_ids = [os.path.splitext(f)[0] for f in os.listdir(image_dir) if f.endswith(\".tif\")]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.file_ids[idx]\n",
    "        img_path = os.path.join(self.image_dir, f\"{image_id}.tif\")\n",
    "        xml_path = os.path.join(self.xml_dir, f\"{image_id}.xml\")\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = image.shape\n",
    "        inst_map, type_map = create_training_maps(xml_path, (h, w))\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, masks=[inst_map, type_map])\n",
    "            image = transformed[\"image\"]\n",
    "            inst_map, type_map = transformed[\"masks\"]\n",
    "\n",
    "        # The model expects a stacked label map\n",
    "        labels = np.stack([inst_map, type_map], axis=0)\n",
    "        return image, torch.from_numpy(labels).long()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Model Training and Validation\n",
    "#\n",
    "# We fine-tune a pre-trained HoVer-Net model. After each epoch, we evaluate on the validation set and save the model if its performance improves.\n",
    "\n",
    "# %%\n",
    "def train_and_validate():\n",
    "    print(\"--- Initializing Model and DataLoaders ---\")\n",
    "    \n",
    "    # --- CORRECTED Model Initialization ---\n",
    "    num_classes = len(CONFIG.CLASSES) + 1  # Add 1 for the background class\n",
    "    # Load pretrained HoVer-Net\n",
    "    model = hovernet.HoverNet.from_pretrained(\"hgsc_v1_efficientnet_b5\", device=torch.device(\"cpu\"))\n",
    "\n",
    "\n",
    "    if model.model.heads[\"type\"][\"nuc_type\"] != num_classes:\n",
    "        model.model.heads[\"type\"][\"nuc_type\"] = num_classes  # update the config\n",
    "        model.model.decoder.heads[\"type\"][\"nuc_type\"] = torch.nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        print(f\"ðŸ”§ Updated 'nuc_type' head to {num_classes} classes.\")\n",
    "        \n",
    "    model.model.to(DEVICE)\n",
    "\n",
    "\n",
    "    # --- Setup Data ---\n",
    "    transform = Compose([\n",
    "        Resize(CONFIG.IMG_SIZE, CONFIG.IMG_SIZE),\n",
    "        Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    train_dataset = NucleiDataset(CONFIG.TRAIN_IMG_DIR, CONFIG.TRAIN_XML_DIR, transform=transform)\n",
    "    val_dataset = NucleiDataset(CONFIG.VAL_IMG_DIR, CONFIG.VAL_XML_DIR, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # --- Optimizer and Training Loop ---\n",
    "    optimizer = optim.Adam(model.model.parameters(), lr=CONFIG.LR)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    print(\"--- Starting Training ---\")\n",
    "    for epoch in range(CONFIG.EPOCHS):\n",
    "        model.model.train()\n",
    "        total_train_loss = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG.EPOCHS} [Train]\"):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # The model computes its own loss when labels are provided\n",
    "            images = images.to(DEVICE)\n",
    "            inst_maps = labels[:, 0, :, :].to(DEVICE)  # instance map\n",
    "            type_maps = labels[:, 1, :, :].to(DEVICE)  # type map\n",
    "\n",
    "            outputs = model.model(images)  # forward pass\n",
    "            pred_inst_maps = outputs.nuc.nuc_map      # predicted instance maps\n",
    "            pred_type_maps = outputs.type.nuc_type     # predicted type maps\n",
    "\n",
    "            # --- Compute losses manually ---\n",
    "            # Use CrossEntropy for type maps\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            loss_type = criterion(pred_type_maps, type_maps)\n",
    "\n",
    "            # For nuclei map, you can implement dice loss / mse / hover loss if available\n",
    "            loss_inst = torch.tensor(0.0, device=DEVICE)  # placeholder if you donâ€™t have hover loss\n",
    "\n",
    "            loss = loss_type + loss_inst\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        # --- Validation Step ---\n",
    "        model.model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{CONFIG.EPOCHS} [Val]\"):\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                images = images.to(DEVICE)\n",
    "                inst_maps = labels[:, 0, :, :].to(DEVICE)  # instance map\n",
    "                type_maps = labels[:, 1, :, :].to(DEVICE)  # type map\n",
    "\n",
    "                outputs = model.model(images)  # forward pass\n",
    "                pred_inst_maps = outputs.nuc.nuc_map       # predicted instance maps\n",
    "                pred_type_maps = outputs.type.nuc_type     # predicted type maps\n",
    "\n",
    "                # --- Compute losses manually ---\n",
    "                # Use CrossEntropy for type maps\n",
    "                criterion = torch.nn.CrossEntropyLoss()\n",
    "                loss_type = criterion(pred_type_maps, type_maps)\n",
    "\n",
    "                # For nuclei map, you can implement dice loss / mse / hover loss if available\n",
    "                loss_inst = torch.tensor(0.0, device=DEVICE)  # placeholder if you donâ€™t have hover loss\n",
    "\n",
    "                loss = loss_type + loss_inst\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), CONFIG.MODEL_SAVE_PATH)\n",
    "            print(f\"âœ… New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "if not os.path.exists(CONFIG.MODEL_SAVE_PATH):\n",
    "    train_and_validate()\n",
    "else:\n",
    "    print(f\"Model already trained. Found at: {CONFIG.MODEL_SAVE_PATH}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Inference and Submission\n",
    "#\n",
    "# We load our best-performing model and run it on the test set to generate the final `submission.csv` file.\n",
    "\n",
    "# %%\n",
    "def run_inference():\n",
    "    print(\"\\n--- Running Full Inference Pipeline on Test Set ---\")\n",
    "    \n",
    "    # --- CORRECTED Model Loading ---\n",
    "    num_classes = len(CONFIG.CLASSES) + 1\n",
    "    model = hovernet.HoverNet.from_pretrained(\"hgsc_v1_efficientnet_b5\", device=torch.device(\"cpu\"))\n",
    "    # If your dataset has a different number of nucleus types, adjust the output head\n",
    "    if model.model.heads[\"type\"][\"nuc_type\"] != num_classes:\n",
    "        model.model.heads[\"type\"][\"nuc_type\"] = num_classes  # update the config\n",
    "        model.model.decoder.heads[\"type\"][\"nuc_type\"] = torch.nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        print(f\"ðŸ”§ Updated 'nuc_type' head to {num_classes} classes.\")\n",
    "    model.load_state_dict(torch.load(CONFIG.MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.set_inference_mode()\n",
    "\n",
    "    # --- Inference Transforms ---\n",
    "    transform = Compose([\n",
    "        Resize(CONFIG.IMG_SIZE, CONFIG.IMG_SIZE),\n",
    "        Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    submission_data = []\n",
    "    test_files = [f for f in os.listdir(CONFIG.TEST_IMG_DIR) if f.endswith(\".tif\")]\n",
    "\n",
    "    for filename in tqdm(test_files, desc=\"Generating predictions\"):\n",
    "        image_id = os.path.splitext(filename)[0]\n",
    "        img_path = os.path.join(CONFIG.TEST_IMG_DIR, filename)\n",
    "        \n",
    "        image_orig = cv2.imread(img_path)\n",
    "        image_orig = cv2.cvtColor(image_orig, cv2.COLOR_BGR2RGB)\n",
    "        h_orig, w_orig, _ = image_orig.shape\n",
    "\n",
    "        # Transform image for model input\n",
    "        image_tensor = transform(image=image_orig)[\"image\"].unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get predictions\n",
    "            prob_map = model.predict(image_tensor)\n",
    "            output = model.post_process(prob_map)[\"nuc\"][0]\n",
    "            pred_inst_map, pred_type_map = output[0], output[1]\n",
    "            \n",
    "            # Resize back to original image size\n",
    "            pred_inst_map = cv2.resize(pred_inst_map.astype(np.uint16), (w_orig, h_orig), interpolation=cv2.INTER_NEAREST)\n",
    "            pred_type_map = cv2.resize(pred_type_map.astype(np.uint8), (w_orig, h_orig), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Convert model output to the required submission format\n",
    "        final_masks = {cls: np.zeros((h_orig, w_orig), dtype=np.int32) for cls in CONFIG.CLASSES}\n",
    "        \n",
    "        for inst_id in range(1, pred_inst_map.max() + 1):\n",
    "            inst_mask = (pred_inst_map == inst_id)\n",
    "            if inst_mask.sum() == 0: continue\n",
    "            \n",
    "            inst_type = np.median(pred_type_map[inst_mask]).astype(int)\n",
    "            \n",
    "            if inst_type in CONFIG.INV_CLASS_MAP:\n",
    "                class_name = CONFIG.INV_CLASS_MAP[inst_type]\n",
    "                final_masks[class_name][inst_mask] = inst_id\n",
    "\n",
    "        row = {\"image_id\": image_id}\n",
    "        for class_name in [\"Epithelial\", \"Lymphocyte\", \"Neutrophil\", \"Macrophage\"]:\n",
    "            row[class_name] = rle_encode_instance_mask(final_masks[class_name])\n",
    "        submission_data.append(row)\n",
    "\n",
    "    # --- Create Submission File ---\n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    submission_df = submission_df[[\"image_id\", \"Epithelial\", \"Lymphocyte\", \"Neutrophil\", \"Macrophage\"]]\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "    print(\"\\nSubmission file 'submission.csv' created successfully!\")\n",
    "    return submission_df\n",
    "\n",
    "submission_df = run_inference()\n",
    "print(\"\\n--- Final Submission Preview ---\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
