{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab26a1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /Users/swooshie/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n",
      "100%|██████████| 170M/170M [00:02<00:00, 64.1MB/s] \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'FastRCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 264\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# 3. Model Initialization (Conceptual)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 264\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_instance_segmentation_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# 4. Conceptual Training Loop (Skipped for brevity, but this is where you handle wPQ optimization)\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# In a real scenario, you would set model.train(), define an optimizer, \u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# use a custom loss or sampler to handle class imbalance, and train for epochs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# 5. Inference on Test Set\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 234\u001b[0m, in \u001b[0;36mget_instance_segmentation_model\u001b[0;34m(num_classes)\u001b[0m\n\u001b[1;32m    232\u001b[0m in_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mroi_heads\u001b[38;5;241m.\u001b[39mbox_predictor\u001b[38;5;241m.\u001b[39mcls_score\u001b[38;5;241m.\u001b[39min_features\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Replace the pre-trained box predictor with one that knows our number of classes\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m model\u001b[38;5;241m.\u001b[39mroi_heads\u001b[38;5;241m.\u001b[39mbox_predictor \u001b[38;5;241m=\u001b[39m \u001b[43mFastRCNN\u001b[49m(in_features, num_classes)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# Get the number of input features for the mask classifier\u001b[39;00m\n\u001b[1;32m    237\u001b[0m in_features_mask \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mroi_heads\u001b[38;5;241m.\u001b[39mmask_predictor\u001b[38;5;241m.\u001b[39mconv5_mask\u001b[38;5;241m.\u001b[39min_channels\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FastRCNN' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch and TorchVision for Deep Learning\n",
    "# Note: For this single-block example, we will use torchvision's Mask R-CNN as a high-level model placeholder.\n",
    "# In a full solution, a customized architecture (like Hover-Net or a multi-task U-Net) would be used.\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "# --- Configuration and File Paths ---\n",
    "# IMPORTANT: Adjust this to your actual Kaggle data directory\n",
    "DATA_DIR = 'kaggle-data'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'test_final')\n",
    "TRAIN_CSV_PATH = os.path.join(DATA_DIR, 'train_ground_truth.csv')\n",
    "SUBMISSION_CSV_PATH = 'submission.csv'\n",
    "\n",
    "# Define the four target classes\n",
    "CLASS_NAMES = ['Epithelial', 'Lymphocyte', 'Macrophage', 'Neutrophil']\n",
    "NUM_CLASSES = len(CLASS_NAMES) + 1 # +1 for background\n",
    "CLASS_TO_ID = {name: i + 1 for i, name in enumerate(CLASS_NAMES)}\n",
    "ID_TO_CLASS = {i + 1: name for i, name in enumerate(CLASS_NAMES)}\n",
    "\n",
    "# Default image size for training/inference. H&E images are large, so they must be tiled.\n",
    "# We will use a smaller size for this conceptual example.\n",
    "TILE_SIZE = (512, 512) \n",
    "\n",
    "# --- RLE Encoding/Decoding Functions (Provided in Description) ---\n",
    "def rle_encode_instance_mask(mask: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Convert an instance segmentation mask (H,W) -> RLE triple string.\n",
    "    0 = background, >0 = instance IDs.\n",
    "    \"\"\"\n",
    "    # Ensure mask is a numpy array of integers\n",
    "    if mask.dtype != np.int32:\n",
    "        pixels = mask.flatten(order=\"F\").astype(np.int32)\n",
    "    else:\n",
    "        pixels = mask.flatten(order=\"F\")\n",
    "\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "\n",
    "    rle = []\n",
    "    for i in range(0, len(runs) - 1):\n",
    "        start = runs[i]\n",
    "        end = runs[i+1] if i+1 < len(runs) else len(pixels)-1\n",
    "        length = end - start\n",
    "        val = pixels[start]\n",
    "        if val > 0:\n",
    "            # val: Instance ID, start: 1-based start index, length: run length\n",
    "            rle.extend([val, start, length])\n",
    "\n",
    "    if not rle:\n",
    "        return \"0\" \n",
    "\n",
    "    return \" \".join(map(str, rle))\n",
    "\n",
    "def rle_decode_instance_mask(rle: str, shape: tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert RLE triple string back into an instance mask of shape (H, W).\n",
    "    \"\"\"\n",
    "    if not rle or str(rle).strip() in (\"\", \"0\", \"nan\"):\n",
    "        return np.zeros(shape, dtype=np.uint16)\n",
    "    try:\n",
    "        s = list(map(int, rle.split()))\n",
    "    except ValueError:\n",
    "        return np.zeros(shape, dtype=np.uint16)\n",
    "        \n",
    "    mask = np.zeros(shape[0]*shape[1], dtype=np.uint16)\n",
    "    for i in range(0, len(s), 3):\n",
    "        val, start, length = s[i], s[i+1], s[i+2]\n",
    "        # start is 1-based, mask is 0-based\n",
    "        mask[start-1:start-1+length] = val \n",
    "    return mask.reshape(shape, order=\"F\")\n",
    "\n",
    "# --- XML Annotation Parser to Generate Instance Masks ---\n",
    "def get_instance_masks_from_xml(xml_path, image_shape, class_to_id):\n",
    "    \"\"\"\n",
    "    Parses the raw XML annotation file to generate a dictionary of \n",
    "    instance masks, one 2D array per cell class.\n",
    "    \n",
    "    Args:\n",
    "        xml_path (str): Path to the .xml file.\n",
    "        image_shape (tuple): (H, W) of the corresponding image.\n",
    "        class_to_id (dict): Mapping from class name to ID.\n",
    "\n",
    "    Returns:\n",
    "        dict: {class_name: 2D numpy array (H, W) of instance masks}\n",
    "    \"\"\"\n",
    "    H, W = image_shape\n",
    "    instance_masks = {name: np.zeros((H, W), dtype=np.uint16) for name in class_to_id.keys()}\n",
    "    \n",
    "    if not os.path.exists(xml_path):\n",
    "        return instance_masks\n",
    "\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for annotation in root.findall('.//Annotation'):\n",
    "        class_name_tag = annotation.find('.//Attribute[@Name]')\n",
    "        if class_name_tag is None:\n",
    "            continue\n",
    "            \n",
    "        class_name = class_name_tag.get('Name')\n",
    "        if class_name not in class_to_id:\n",
    "            continue\n",
    "            \n",
    "        class_id = class_to_id[class_name]\n",
    "        instance_counter = 1\n",
    "\n",
    "        for region in annotation.findall('.//Region'):\n",
    "            vertices = region.findall('.//Vertex')\n",
    "            if not vertices:\n",
    "                continue\n",
    "                \n",
    "            # Extract polygon vertices\n",
    "            polygon = []\n",
    "            for vertex in vertices:\n",
    "                # Vertices are given as floating-point in the XML, convert to int\n",
    "                x = int(float(vertex.get('X')))\n",
    "                y = int(float(vertex.get('Y')))\n",
    "                polygon.append((x, y))\n",
    "            \n",
    "            # Draw the polygon mask\n",
    "            if polygon:\n",
    "                # cv2.fillPoly expects an array of polygons, each polygon being an array of points\n",
    "                poly_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "                cv2.fillPoly(poly_mask, [np.array(polygon)], color=instance_counter)\n",
    "                \n",
    "                # Add this instance to the class-specific mask array\n",
    "                # Ensure no overlap with existing instances in this class mask (shouldn't happen with correct XML)\n",
    "                # We use bitwise OR to combine new instance with existing ones\n",
    "                current_class_mask = instance_masks[class_name]\n",
    "                current_class_mask[poly_mask > 0] = instance_counter\n",
    "                \n",
    "                instance_counter += 1\n",
    "                \n",
    "    return instance_masks\n",
    "\n",
    "# --- PyTorch Dataset for Training ---\n",
    "class NucleiDataset(Dataset):\n",
    "    def __init__(self, image_ids, base_dir, xml_dir, transforms=None):\n",
    "        self.image_ids = image_ids\n",
    "        self.base_dir = base_dir\n",
    "        self.xml_dir = xml_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_path = os.path.join(self.base_dir, f'{image_id}.tif')\n",
    "        xml_path = os.path.join(self.xml_dir, f'{image_id}.xml')\n",
    "\n",
    "        # Load image (H, W, C)\n",
    "        image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "        H, W, C = image.shape\n",
    "        \n",
    "        # NOTE on large images: Real H&E images are often WSI (Whole Slide Images) \n",
    "        # and would need to be tiled/patched here to fit into GPU memory. \n",
    "        # For simplicity, we assume the provided .tif files are small enough, but \n",
    "        # this is a critical simplification for a real project.\n",
    "\n",
    "        # Get all instance masks for this image\n",
    "        instance_masks_per_class = get_instance_masks_from_xml(xml_path, (H, W), CLASS_TO_ID)\n",
    "\n",
    "        # Combine all class masks into a single set of targets for an instance segmentation model\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "\n",
    "        # Instance ID must be unique across all classes in the model target format.\n",
    "        # We process them class by class.\n",
    "        for class_name, class_id in CLASS_TO_ID.items():\n",
    "            class_mask = instance_masks_per_class[class_name]\n",
    "            \n",
    "            # Find unique instance IDs (excluding background 0)\n",
    "            instance_ids = np.unique(class_mask)\n",
    "            instance_ids = instance_ids[instance_ids != 0]\n",
    "\n",
    "            for inst_id in instance_ids:\n",
    "                mask = (class_mask == inst_id).astype(np.uint8)\n",
    "                \n",
    "                # Get bounding box from mask\n",
    "                pos = np.where(mask)\n",
    "                xmin = np.min(pos[1])\n",
    "                xmax = np.max(pos[1])\n",
    "                ymin = np.min(pos[0])\n",
    "                ymax = np.max(pos[0])\n",
    "\n",
    "                if xmax > xmin and ymax > ymin:\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    labels.append(class_id)\n",
    "                    masks.append(mask)\n",
    "\n",
    "        # Convert to Tensors\n",
    "        image = torch.as_tensor(image, dtype=torch.float32).permute(2, 0, 1) # C, H, W\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        target[\"masks\"] = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "\n",
    "        # Apply transforms (e.g., color normalization, augmentation)\n",
    "        if self.transforms:\n",
    "             # Using torchvision's transform setup requires a function that takes (image, target)\n",
    "             # and returns (image, target). A full implementation would use Albumentations \n",
    "             # for more robust medical image augmentation.\n",
    "             pass # Simplified for this example\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# --- Model Definition (Mask R-CNN Placeholder) ---\n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # Load a pre-trained Mask R-CNN model (e.g., ResNet50-FPN backbone)\n",
    "    # The weights are pre-trained on COCO dataset, which is a good starting point.\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights.COCO_V1)\n",
    "\n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # Replace the pre-trained box predictor with one that knows our number of classes\n",
    "    model.roi_heads.box_predictor = FastRCNN(in_features, num_classes)\n",
    "\n",
    "    # Get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    # Replace the pre-trained mask predictor with one that knows our number of classes\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       model.roi_heads.mask_predictor.conv5_mask.out_channels,\n",
    "                                                       num_classes)\n",
    "    return model\n",
    "\n",
    "# --- Utility: Collate Function for DataLoader ---\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == '__main__':\n",
    "    # 1. Setup Data Paths and IDs\n",
    "    train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "    all_image_ids = train_df['image_id'].tolist()\n",
    "    \n",
    "    # Simple split for conceptual training/validation\n",
    "    train_ids, val_ids = train_test_split(all_image_ids, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # 2. Setup Dataset and DataLoader\n",
    "    # NOTE: The full dataset is very large. This uses a conceptual subset/setup.\n",
    "    train_xml_dir = os.path.join(TRAIN_DIR, 'annotations') # Assuming XMLs are in 'train/annotations'\n",
    "    train_dataset = NucleiDataset(train_ids, TRAIN_DIR, train_xml_dir)\n",
    "    \n",
    "    # 3. Model Initialization (Conceptual)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = get_instance_segmentation_model(NUM_CLASSES)\n",
    "    model.to(device)\n",
    "    \n",
    "    # 4. Conceptual Training Loop (Skipped for brevity, but this is where you handle wPQ optimization)\n",
    "    # In a real scenario, you would set model.train(), define an optimizer, \n",
    "    # use a custom loss or sampler to handle class imbalance, and train for epochs.\n",
    "    # Training code:\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    # train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "    # ... train loop ...\n",
    "    \n",
    "    # 5. Inference on Test Set\n",
    "    test_image_paths = sorted(glob.glob(os.path.join(TEST_DIR, '*.tif')))\n",
    "    test_image_ids = [os.path.basename(p).replace('.tif', '') for p in test_image_paths]\n",
    "\n",
    "    submission_rows = []\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Starting inference on {len(test_image_ids)} test images...\")\n",
    "    for image_id, image_path in tqdm(zip(test_image_ids, test_image_paths), total=len(test_image_ids)):\n",
    "        try:\n",
    "            image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "        except FileNotFoundError:\n",
    "            # Handle case where file might be missing or path is wrong\n",
    "            print(f\"Image file not found for {image_id}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        H, W, C = image.shape\n",
    "        # Preprocessing: Convert to PyTorch tensor (C, H, W) and normalize (conceptual)\n",
    "        input_tensor = torch.as_tensor(image, dtype=torch.float32).permute(2, 0, 1).to(device)\n",
    "        input_tensor = input_tensor / 255.0 # Simple normalization\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            prediction = model([input_tensor])\n",
    "\n",
    "        if not prediction or not prediction[0]:\n",
    "            # If no predictions, all RLEs are \"0\"\n",
    "            rle_results = {name: \"0\" for name in CLASS_NAMES}\n",
    "        else:\n",
    "            pred = prediction[0]\n",
    "            # Use a threshold for mask/detection confidence (e.g., 0.5)\n",
    "            # and a mask threshold (e.g., 0.5) to get binary masks\n",
    "            keep = pred['scores'] > 0.5\n",
    "            \n",
    "            masks = pred['masks'][keep].squeeze(1) # [N, H, W]\n",
    "            labels = pred['labels'][keep]\n",
    "            \n",
    "            # The output masks are floats (logits), convert to binary\n",
    "            masks = (masks > 0.5).cpu().numpy().astype(np.uint8)\n",
    "\n",
    "            rle_results = {name: \"0\" for name in CLASS_NAMES}\n",
    "            \n",
    "            # Aggregate instance masks by class\n",
    "            for class_idx in range(1, NUM_CLASSES):\n",
    "                class_name = ID_TO_CLASS[class_idx]\n",
    "                \n",
    "                # Filter masks and labels for the current class\n",
    "                class_indices = (labels == class_idx).nonzero(as_tuple=True)[0]\n",
    "                \n",
    "                if len(class_indices) > 0:\n",
    "                    class_masks = masks[class_indices]\n",
    "                    \n",
    "                    # Create the final class-specific instance map\n",
    "                    # The value of each pixel must be the *unique instance ID* (1, 2, 3...)\n",
    "                    class_instance_map = np.zeros((H, W), dtype=np.uint16)\n",
    "                    \n",
    "                    for i, mask in enumerate(class_masks):\n",
    "                        # i + 1 is the unique instance ID for this class\n",
    "                        instance_id = i + 1\n",
    "                        class_instance_map[mask > 0] = instance_id \n",
    "\n",
    "                    # Convert the instance map to RLE string\n",
    "                    rle_string = rle_encode_instance_mask(class_instance_map)\n",
    "                    rle_results[class_name] = rle_string\n",
    "\n",
    "        # Collect results for the submission DataFrame\n",
    "        row = {'image_id': image_id}\n",
    "        row.update(rle_results)\n",
    "        submission_rows.append(row)\n",
    "        \n",
    "    # 6. Generate Submission CSV\n",
    "    submission_df = pd.DataFrame(submission_rows)\n",
    "    \n",
    "    # Crucial Step: Sort the submission by image_id lexicographically\n",
    "    submission_df = submission_df.sort_values(by='image_id').reset_index(drop=True)\n",
    "    \n",
    "    # Re-order columns to match required submission format: image_id,Epithelial,Lymphocyte,Neutrophil,Macrophage\n",
    "    # Note the specific order of Neutrophil and Macrophage.\n",
    "    final_cols = ['image_id', 'Epithelial', 'Lymphocyte', 'Neutrophil', 'Macrophage']\n",
    "    submission_df = submission_df[final_cols]\n",
    "    \n",
    "    # Replace any NaN RLEs with the required string \"0\"\n",
    "    submission_df = submission_df.fillna(\"0\")\n",
    "    \n",
    "    # Save the final submission file\n",
    "    submission_df.to_csv(SUBMISSION_CSV_PATH, index=False)\n",
    "    print(f\"\\nSubmission file created: {SUBMISSION_CSV_PATH}\")\n",
    "    print(\"\\nFirst few rows of submission:\")\n",
    "    print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
